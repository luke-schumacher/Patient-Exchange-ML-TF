{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91a6689-efe7-41ba-9ee2-afa668d26754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36d62a4-3ba4-47cf-a556-24b4829ab219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- DATA FUNCTIONS -----------------------------------\n",
    "\n",
    "RAW_STRINGS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "STRINGS = RAW_STRINGS + RAW_STRINGS + RAW_STRINGS\n",
    "\n",
    "# CHAR_TO_INT = dict(zip([item for item in RAW_STRINGS], range(len(RAW_STRINGS))))\n",
    "CHAR_TO_INT = dict(zip([item for item in RAW_STRINGS], range(1, len(RAW_STRINGS) + 1)))\n",
    "INT_TO_CHAR = dict(zip(list(CHAR_TO_INT.values()), list(CHAR_TO_INT.keys())))\n",
    "\n",
    "START_TOKEN = \"0\"\n",
    "END_TOKEN = \"1\"\n",
    "\n",
    "def generate_data(data_size=100):\n",
    "    \n",
    "    def generate_cond_sequence(condition):\n",
    "        for i in range(len(RAW_STRINGS)):\n",
    "            if condition == RAW_STRINGS[i]:\n",
    "                \n",
    "                random_number = np.random.choice([0, 1, 2])\n",
    "                \n",
    "                if random_number == 0:\n",
    "                    length = 3\n",
    "                elif random_number == 1:\n",
    "                    length = 6\n",
    "                elif random_number == 2:\n",
    "                    length = 10\n",
    "                else:\n",
    "                    raise NotImplementedError()\n",
    "                \n",
    "                return STRINGS[i:i+length]\n",
    "        \n",
    "        return RAW_STRINGS[::-1][:10]\n",
    "    \n",
    "    conditions_list = []\n",
    "    sequences_lists = []\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        condition = np.random.choice([item for item in RAW_STRINGS])[0]\n",
    "        \n",
    "        seq = generate_cond_sequence(condition)\n",
    "        \n",
    "        conditions_list.append(condition)\n",
    "        sequences_lists.append(seq)\n",
    "        \n",
    "    return conditions_list, sequences_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2600d879-cca7-4d62-8ea6-ffc729fa0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_tokens(conds, seqs):\n",
    "    new_seqs = []\n",
    "    for seq in seqs:\n",
    "        new_seqs.append(START_TOKEN + seq + END_TOKEN)\n",
    "        \n",
    "    return conds, new_seqs\n",
    "    \n",
    "    \n",
    "def make_same_length(conds, seqs):\n",
    "    new_seqs = []\n",
    "    \n",
    "    max_length = np.max([len(seq) for seq in seqs])\n",
    "    \n",
    "    for seq in seqs:\n",
    "        remaining_tokens = max_length - len(seq)\n",
    "        new_seqs.append(seq + \"\".join([END_TOKEN]*remaining_tokens))\n",
    "        \n",
    "    return conds, new_seqs\n",
    "\n",
    "\n",
    "def make_training_data(conds, seqs):\n",
    "    \n",
    "    input_seqs = []\n",
    "    \n",
    "    for seq in seqs:\n",
    "        input_seqs.append(seq[:-1])\n",
    "    \n",
    "    input_data = (conds, input_seqs)\n",
    "    \n",
    "    output_seqs = []\n",
    "    \n",
    "    for seq in seqs:\n",
    "        output_seqs.append(seq[1:])\n",
    "        \n",
    "    output_data = output_seqs\n",
    "    \n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464e3a43-9fba-48b7-b886-90eb9d2291bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionMapper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.integer_map = CHAR_TO_INT.copy()\n",
    "        self.string_map = INT_TO_CHAR.copy()\n",
    "        \n",
    "        # self.dimension = len(self.integer_map)\n",
    "        self.dimension = len(self.integer_map) + 1\n",
    "        \n",
    "    def map_to_ints(self, input_string):\n",
    "        input_numbers = [self.integer_map[item] for item in input_string]\n",
    "        \n",
    "        return np.asarray(input_numbers)\n",
    "    \n",
    "    def map_list_to_ints_vectors(self, list_of_strings):\n",
    "        input_length = len(list_of_strings)\n",
    "        \n",
    "        vectors = []\n",
    "        \n",
    "        for input_string in list_of_strings:\n",
    "            vectors.append(self.map_to_ints(input_string))\n",
    "            \n",
    "        vectors = np.asarray(vectors)\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    def map_ints_to_string(self, input_ints):\n",
    "        string = \"\"\n",
    "        \n",
    "        for integer in input_ints:\n",
    "            string += self.string_map[integer]\n",
    "            \n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274cf379-1295-4e82-81d5-bdafdb70b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqMapper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.integer_map = CHAR_TO_INT.copy()\n",
    "        \n",
    "        n_ints = len(self.integer_map)\n",
    "        \n",
    "        # self.integer_map.update({START_TOKEN: n_ints, END_TOKEN: n_ints + 1})\n",
    "        # self.dimension = len(self.integer_map)\n",
    "        \n",
    "        self.integer_map.update({START_TOKEN: n_ints + 1, END_TOKEN: n_ints + 2})\n",
    "        \n",
    "        self.dimension = len(self.integer_map) + 1\n",
    "        \n",
    "        self.string_map = dict(zip(list(self.integer_map.values()), list(self.integer_map.keys())))\n",
    "    \n",
    "    def map_to_ints(self, input_string):\n",
    "        input_numbers = [self.integer_map[item] for item in input_string]\n",
    "        \n",
    "        return np.asarray(input_numbers)\n",
    "    \n",
    "    def map_list_to_ints_vectors(self, list_of_strings):\n",
    "        input_length = len(list_of_strings)\n",
    "        \n",
    "        vectors = []\n",
    "        \n",
    "        for input_string in list_of_strings:\n",
    "            vectors.append(self.map_to_ints(input_string))\n",
    "            \n",
    "        vectors = np.asarray(vectors)\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    def map_ints_to_string(self, input_ints):\n",
    "        string = \"\"\n",
    "        \n",
    "        for integer in input_ints:\n",
    "            string += self.string_map[integer]\n",
    "            \n",
    "        return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9eaa06-b19f-4e12-807c-2af1f9c35ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_training_data(input_train, output_train, mappers):\n",
    "    converted_input_train = mappers[0].map_list_to_ints_vectors(input_train[0]), mappers[1].map_list_to_ints_vectors(input_train[1])\n",
    "    converted_output_train = mappers[1].map_list_to_ints_vectors(output_train)\n",
    "    \n",
    "    return converted_input_train, converted_output_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d26f0f4a-0b95-42f3-89cd-50b4d6fdfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- TENSORFLOW LAYERS AND MODELS -----------------------------------\n",
    "    \n",
    "# --------------------------------------- masked layers\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d18f78f-f364-4b09-a469-eb94a19f93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- positional embedding\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]  # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)  # (1, depth)\n",
    "    angle_rads = positions * angle_rates  # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size,\n",
    "                 d_model,\n",
    "                 use_embedding=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_embedding = use_embedding\n",
    "\n",
    "        if self.use_embedding:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Dense(d_model, activation=\"relu\")\n",
    "\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        if self.use_embedding:\n",
    "            return self.embedding.compute_mask(*args, **kwargs)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positional_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c8cda8-ca1f-4773-8a22-7c3a5999c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- attention layers\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask=True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttentionFeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1,\n",
    "                 attention=\"global\"):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if attention == \"global\":\n",
    "            self.self_attention = GlobalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        elif attention == \"causal\":\n",
    "            self.self_attention = CausalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        else:\n",
    "            raise NotImplemented(f\"The choice {attention} for attention is not implemented.\")\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttentionCrossAttentionFeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 dropout_rate=0.1,\n",
    "                 attention=\"causal\"):\n",
    "\n",
    "        super(SelfAttentionCrossAttentionFeedForwardLayer, self).__init__()\n",
    "\n",
    "        if attention == \"global\":\n",
    "            self.self_attention = GlobalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        elif attention == \"causal\":\n",
    "            self.self_attention = CausalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        else:\n",
    "            raise NotImplemented(f\"The choice {attention} for attention is not implemented.\")\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x\n",
    "    \n",
    "# --------------------------------------- encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, *, \n",
    "                 num_layers, \n",
    "                 d_model, \n",
    "                 num_heads,\n",
    "                 dff, \n",
    "                 vocab_size, \n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            SelfAttentionFeedForwardLayer(d_model=d_model,\n",
    "                                          num_heads=num_heads,\n",
    "                                          dff=dff,\n",
    "                                          dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "# --------------------------------------- decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            SelfAttentionCrossAttentionFeedForwardLayer(d_model=d_model, num_heads=num_heads,\n",
    "                                                        dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        return x # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068df023-b466-4960-af54-4819cb6cd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, \n",
    "                 num_layers, \n",
    "                 d_model, \n",
    "                 num_heads, \n",
    "                 dff,\n",
    "                 input_vocab_size, \n",
    "                 target_vocab_size, \n",
    "                 dropout_rate=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, \n",
    "                               d_model=d_model,\n",
    "                               num_heads=num_heads, \n",
    "                               dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               d_model=d_model,\n",
    "                               num_heads=num_heads, \n",
    "                               dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.Sequential([tf.keras.layers.Dense(target_vocab_size)])\n",
    "        \n",
    "    def save_model_weights(self, save_folder):\n",
    "        self.encoder.save_weights(os.path.join(save_folder, \"encoder.weights.h5\"))\n",
    "        self.decoder.save_weights(os.path.join(save_folder, \"decoder.weights.h5\"))\n",
    "        self.final_layer.save_weights(os.path.join(save_folder, \"final_layer.weights.h5\"))\n",
    "\n",
    "    def load_model_from_weights(self, save_folder):\n",
    "        self.encoder.load_weights(os.path.join(save_folder, \"encoder.weights.h5\"))\n",
    "        self.decoder.load_weights(os.path.join(save_folder, \"decoder.weights.h5\"))\n",
    "        self.final_layer.load_weights(os.path.join(save_folder, \"final_layer.weights.h5\"))\n",
    "\n",
    "    def get_models(self):\n",
    "        return self.encoder, self.decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, x = inputs\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb08698e-a921-4dec-af54-e2b24ff04a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- translator\n",
    "class Translator(tf.Module):\n",
    "    def __init__(self, transformer, mappers, sample=True, min_prob=0.05):\n",
    "        self.cond_mapper = mappers[0]\n",
    "        self.seq_mapper = mappers[1]\n",
    "        self.transformer = transformer\n",
    "        \n",
    "        self.sample = sample\n",
    "        \n",
    "        assert 0 <= min_prob < 1\n",
    "        \n",
    "        self.min_prob = min_prob\n",
    "\n",
    "    def __call__(self, cond_seq, max_length=15):\n",
    "        if not isinstance(cond_seq, tf.Tensor):\n",
    "            cond_seq = tf.convert_to_tensor(cond_seq)\n",
    "\n",
    "        encoder_input = tf.expand_dims(cond_seq, 0)\n",
    "        \n",
    "        seq_start = self.seq_mapper.integer_map[START_TOKEN]\n",
    "        seq_end = self.seq_mapper.integer_map[END_TOKEN]\n",
    "\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, seq_start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer([encoder_input, tf.expand_dims(output, 0)], training=False)\n",
    "\n",
    "            predictions = predictions[0, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "            \n",
    "            if self.sample:\n",
    "                # calculate and transform probabilities\n",
    "                probabilities = tf.math.softmax(predictions)\n",
    "                high_probs = tf.where(probabilities > self.min_prob, probabilities, 0)[0]\n",
    "                high_probs = high_probs / tf.math.reduce_sum(high_probs)\n",
    "                \n",
    "                # print(high_probs)\n",
    "                \n",
    "                # sample next id\n",
    "                predicted_id = np.random.choice(len(high_probs), size=1, p=high_probs.numpy())[0]\n",
    "            else:\n",
    "                # take id with highest probability\n",
    "                predicted_id = tf.argmax(predictions, axis=-1)[0]\n",
    "\n",
    "            output_array = output_array.write(i + 1, predicted_id)\n",
    "\n",
    "            if predicted_id == seq_end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "\n",
    "        text = self.seq_mapper.map_ints_to_string(output.numpy())  # Shape: `()`.\n",
    "\n",
    "        self.transformer([encoder_input, tf.expand_dims(output[:-1], 0)], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90bf018-5c36-42cc-8c72-e157f5dae1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- main use case\n",
    "def main(train=True, \n",
    "         compute_results=True,\n",
    "         n_conds=-1,\n",
    "         n_samples=3,\n",
    "         epochs=1, \n",
    "         checkpoint_name = \"test_transformer\"):\n",
    "    \n",
    "    # get data\n",
    "    conditions, sequences = generate_data(data_size=5000)\n",
    "    \n",
    "    with open(os.path.join(checkpoint_name, \"raw_data.txt\"), \"w\") as f:\n",
    "        print_dict = {}\n",
    "        for i, cond in enumerate(conditions):\n",
    "            if cond not in print_dict:\n",
    "                print_dict[cond] = []\n",
    "            \n",
    "            print_dict[cond].append(sequences[i])\n",
    "                \n",
    "        with redirect_stdout(f):\n",
    "            # print(\"conditions:\")\n",
    "            # pprint(conditions)\n",
    "            # print(\"\\n\")\n",
    "            # print(\"sequences\")\n",
    "            # pprint(sequences)\n",
    "            \n",
    "            pprint(print_dict)\n",
    "                \n",
    "    conditions, sequences = add_start_end_tokens(conditions, sequences)\n",
    "    conditions, sequences = make_same_length(conditions, sequences)\n",
    "    \n",
    "    with open(os.path.join(checkpoint_name, \"train_data.txt\"), \"w\") as f:\n",
    "        print_dict = {}\n",
    "        for i, cond in enumerate(conditions):\n",
    "            if cond not in print_dict:\n",
    "                print_dict[cond] = []\n",
    "            \n",
    "            print_dict[cond].append(sequences[i])\n",
    "                \n",
    "        with redirect_stdout(f):\n",
    "            # print(\"conditions:\")\n",
    "            # pprint(conditions)\n",
    "            # print(\"\\n\")\n",
    "            # print(\"sequences\")\n",
    "            # pprint(sequences)\n",
    "            \n",
    "            pprint(print_dict)\n",
    "    \n",
    "    cond_mapper = ConditionMapper()\n",
    "    seq_mapper = SeqMapper()\n",
    "    \n",
    "    # generate training data\n",
    "    raw_model_input_data, raw_model_output_data = make_training_data(conditions, sequences)\n",
    "    model_input_data, model_output_data = convert_training_data(raw_model_input_data, \n",
    "                                                                raw_model_output_data, \n",
    "                                                                mappers=(cond_mapper, seq_mapper))\n",
    "    \n",
    "    # build transformer\n",
    "    \n",
    "    # tf example\n",
    "    # num_layers = 4\n",
    "    # d_model = 128\n",
    "    # dff = 512\n",
    "    # num_heads = 8\n",
    "    # dropout_rate = 0.1\n",
    "    \n",
    "    transformer = Transformer(\n",
    "        num_layers=3,\n",
    "        d_model=32,\n",
    "        num_heads=8,\n",
    "        dff=128,\n",
    "        input_vocab_size=cond_mapper.dimension,\n",
    "        target_vocab_size=seq_mapper.dimension,\n",
    "        dropout_rate=0.1,\n",
    "    )\n",
    "    \n",
    "    transformer(model_input_data)\n",
    "    transformer.summary()\n",
    "    \n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        os.mkdir(checkpoint_name)\n",
    "    \n",
    "    if os.path.exists(os.path.join(checkpoint_name, \"encoder.weights.h5\")):\n",
    "        transformer.load_model_from_weights(checkpoint_name)\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # compile transformer\n",
    "    transformer.compile(\n",
    "        loss=masked_loss,\n",
    "        optimizer='Adam',\n",
    "        metrics=[masked_accuracy])\n",
    "    \n",
    "    if train:\n",
    "        print(\"---------------------------- Training model ----------------------------\")\n",
    "        \n",
    "        # fit transformer on batches\n",
    "        transformer.fit(model_input_data,\n",
    "                        model_output_data,\n",
    "                        epochs=epochs,\n",
    "                        # validation_data=val_batches\n",
    "                        )\n",
    "        \n",
    "        transformer.save_model_weights(checkpoint_name)\n",
    "    \n",
    "    # build translator\n",
    "    translator = Translator(transformer, mappers=(cond_mapper, seq_mapper))\n",
    "    \n",
    "    # translate examples\n",
    "    \n",
    "    if compute_results:\n",
    "        print(\"---------------------------- Using model ----------------------------\")\n",
    "            \n",
    "        translation_results = dict()\n",
    "        \n",
    "        if n_conds == -1:\n",
    "            strings_to_use = RAW_STRINGS\n",
    "        elif 0 < n_conds < len(RAW_STRINGS):\n",
    "            strings_to_use = RAW_STRINGS[:n_conds]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        for char in RAW_STRINGS:\n",
    "            \n",
    "            print(f\"Generating output for condition {char}\")\n",
    "            \n",
    "            translation_results[char] = []\n",
    "            example_cond = cond_mapper.map_to_ints(char)\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                model_text, _ = translator(example_cond)\n",
    "                \n",
    "                print(model_text)\n",
    "                \n",
    "                translation_results[char].append(model_text)\n",
    "        \n",
    "        with open(os.path.join(checkpoint_name, \"results.txt\"), \"w\") as f:\n",
    "            with redirect_stdout(f):\n",
    "                pprint(translation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e468962-6017-4a1f-9ee3-646f67575dce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_transformer\\\\raw_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcompute_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[43mn_conds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m         \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(train, compute_results, n_conds, n_samples, epochs, checkpoint_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m      3\u001b[0m          compute_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m          n_conds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# get data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     conditions, sequences \u001b[38;5;241m=\u001b[39m generate_data(data_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw_data.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m         print_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, cond \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(conditions):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m     )\n\u001b[1;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_transformer\\\\raw_data.txt'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(train=True, \n",
    "         compute_results=True, \n",
    "         n_conds=-1,\n",
    "         n_samples=10,\n",
    "         epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
