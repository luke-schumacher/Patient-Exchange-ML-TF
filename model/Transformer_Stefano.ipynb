{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "e91a6689-efe7-41ba-9ee2-afa668d26754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "tensorflow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "from contextlib import redirect_stdout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Output library versions\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "bb1b001d-c4b2-4ec0-af1c-e060788fe8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated encoding legend\n",
    "ENCODING_LEGEND = {\n",
    "    'MRI_CCS_11': 1, 'MRI_EXU_95': 2, 'MRI_FRR_18': 3, 'MRI_FRR_257': 4,\n",
    "    'MRI_FRR_264': 5, 'MRI_FRR_3': 6, 'MRI_FRR_34': 7, 'MRI_MPT_1005': 8,\n",
    "    'MRI_MSR_100': 9, 'MRI_MSR_104': 10, 'MRI_MSR_21': 11, 'MRI_MSR_34': 12,\n",
    "    'START': 13,  # Start token\n",
    "    'END': 14     # End token\n",
    "}\n",
    "\n",
    "CHAR_TO_INT = {\n",
    "    '0': 0,\n",
    "    '1': 1,\n",
    "    '2': 2,\n",
    "    '3': 3,\n",
    "    '4': 4,\n",
    "    '5': 5,\n",
    "    '6': 6,\n",
    "    '7': 7,\n",
    "    '8': 8,\n",
    "    '9': 9,\n",
    "    '10': 10,\n",
    "    '11': 11,\n",
    "    '12': 12,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "f36d62a4-3ba4-47cf-a556-24b4829ab219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- DATA FUNCTIONS -----------------------------------\n",
    "\n",
    "# Updated start and end tokens\n",
    "START_TOKEN = 13\n",
    "END_TOKEN = 14\n",
    "\n",
    "def generate_data(data_size=100):\n",
    "    \"\"\"\n",
    "    Generate synthetic sourceID data sequences.\n",
    "    Each sequence starts with the START token (13) and ends with the END token (14).\n",
    "    Random sourceIDs (from 1 to 12) are included in between.\n",
    "    \"\"\"\n",
    "    \n",
    "    def generate_cond_sequence(condition):\n",
    "        \"\"\"\n",
    "        Generate a sequence of sourceIDs based on a condition.\n",
    "        The condition is an integer (sourceID) between 1 and 12.\n",
    "        \"\"\"\n",
    "        if condition < 1 or condition > 12:\n",
    "            raise ValueError(\"Condition must be between 1 and 12.\")\n",
    "\n",
    "        random_number = np.random.choice([0, 1, 2])\n",
    "\n",
    "        if random_number == 0:\n",
    "            length = 3\n",
    "        elif random_number == 1:\n",
    "            length = 6\n",
    "        elif random_number == 2:\n",
    "            length = 10\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # Generate random sourceIDs for the sequence\n",
    "        return np.random.randint(1, 13, size=length).tolist()\n",
    "\n",
    "    \n",
    "    sequences_lists = []\n",
    "\n",
    "    for i in range(data_size):\n",
    "        # Randomly select a condition (sourceID between 1 and 12)\n",
    "        condition = np.random.randint(1, 12)\n",
    "\n",
    "        # Generate a sequence based on the condition\n",
    "        seq = generate_cond_sequence(condition)\n",
    "\n",
    "        # Add start and end tokens to the sequence\n",
    "        seq = [START_TOKEN] + seq + [END_TOKEN]\n",
    "\n",
    "        \n",
    "        sequences_lists.append(seq)\n",
    "\n",
    "    return sequences_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "2600d879-cca7-4d62-8ea6-ffc729fa0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_tokens(seqs):\n",
    "    new_seqs = [[START_TOKEN] + seq + [END_TOKEN] for seq in seqs]\n",
    "    return new_seqs\n",
    "\n",
    "    \n",
    "    \n",
    "def make_same_length(seqs):\n",
    "    max_length = max(len(seq) for seq in seqs)\n",
    "    new_seqs = [seq + [END_TOKEN] * (max_length - len(seq)) for seq in seqs]\n",
    "    return new_seqs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_training_data(seqs):\n",
    "    input_seqs = [seq[:-1] for seq in seqs]\n",
    "    output_seqs = [seq[1:] for seq in seqs]\n",
    "    return input_seqs, output_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "464e3a43-9fba-48b7-b886-90eb9d2291bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings\n",
    "RAW_STRINGS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "CHAR_TO_INT = {str(i): i for i in range(11)}  # Includes 0 to 10\n",
    "INT_TO_CHAR = {v: k for k, v in CHAR_TO_INT.items()}\n",
    "\n",
    "class ConditionMapper:\n",
    "    def __init__(self):\n",
    "        # Define your mappings as before\n",
    "        self.integer_map = ENCODING_LEGEND.copy()  # Your original map\n",
    "        self.string_map = {v: k for k, v in self.integer_map.items()}  # Reverse mapping for integer to string\n",
    "        self.dimension = len(self.integer_map) + 1  # Account for possible 'END' token\n",
    "\n",
    "    def map_to_ints(self, input_string):\n",
    "        # Check if the input is a string (e.g., 'MRI_MSR_100') or an integer (e.g., 10)\n",
    "        if isinstance(input_string, int):\n",
    "            # Check if the integer exists in the reverse mapping (for keys like 10 corresponding to 'MRI_MSR_100')\n",
    "            input_string = self.string_map.get(input_string, None)\n",
    "            if input_string is None:\n",
    "                raise KeyError(f\"Integer '{input_string}' not found in string map\")\n",
    "        \n",
    "        # Now input_string should be a valid string that can be mapped to an integer\n",
    "        if input_string not in self.integer_map:\n",
    "            raise KeyError(f\"Character '{input_string}' not found in integer map\")\n",
    "        \n",
    "        return np.array([self.integer_map[input_string]])\n",
    "\n",
    "    def map_list_to_ints_vectors(self, list_of_strings):\n",
    "        return np.array([self.map_to_ints(input_string) for input_string in list_of_strings])\n",
    "\n",
    "    def map_ints_to_string(self, input_ints):\n",
    "        return \"\".join([self.string_map.get(integer, '') for integer in input_ints])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "274cf379-1295-4e82-81d5-bdafdb70b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqMapper:\n",
    "    def __init__(self):\n",
    "        self.integer_map = ENCODING_LEGEND.copy()\n",
    "        n_ints = len(self.integer_map)\n",
    "        \n",
    "        # Add START and END tokens\n",
    "        self.integer_map['START'] = START_TOKEN\n",
    "        self.integer_map['END'] = END_TOKEN\n",
    "        \n",
    "        #Update dimension (max index + 1 for vocabulary size)\n",
    "        self.dimension = max(self.integer_map.values()) + 1\n",
    "\n",
    "        # Reverse mapping\n",
    "        self.string_map = {v: k for k, v in self.integer_map.items()}\n",
    "\n",
    "    \n",
    "    def map_to_ints(self, input_string):\n",
    "    # Handle integer inputs\n",
    "        if isinstance(input_string, int):\n",
    "            mapped_string = self.string_map.get(input_string)\n",
    "            if mapped_string is None:\n",
    "                raise KeyError(f\"Integer '{input_string}' not found in string map\")\n",
    "            input_string = mapped_string\n",
    "\n",
    "    # Ensure input_string is now a valid string\n",
    "        if input_string not in self.integer_map:\n",
    "            raise KeyError(f\"Character '{input_string}' not found in integer map\")\n",
    "    \n",
    "        return np.array([self.integer_map[input_string]])\n",
    "\n",
    "\n",
    "    \n",
    "    def map_list_to_ints_vectors(self, list_of_strings):\n",
    "        input_length = len(list_of_strings)\n",
    "        \n",
    "        vectors = []\n",
    "        \n",
    "        for input_string in list_of_strings:\n",
    "            vectors.append(self.map_to_ints(input_string))\n",
    "            \n",
    "        vectors = np.asarray(vectors)\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    def map_ints_to_string(self, input_ints):\n",
    "        string = \"\"\n",
    "        \n",
    "        for integer in input_ints:\n",
    "            string += self.string_map[integer]\n",
    "            \n",
    "        return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "bf9eaa06-b19f-4e12-807c-2af1f9c35ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_training_data(input_train, output_train, mapper):\n",
    "    seq_mapper = mapper\n",
    "    converted_input_train = seq_mapper.map_list_to_ints_vectors(input_train)\n",
    "    converted_output_train = seq_mapper.map_list_to_ints_vectors(output_train)\n",
    "    return converted_input_train, converted_output_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "d26f0f4a-0b95-42f3-89cd-50b4d6fdfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- TENSORFLOW LAYERS AND MODELS -----------------------------------\n",
    "    \n",
    "# --------------------------------------- masked layers\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "7d18f78f-f364-4b09-a469-eb94a19f93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- positional embedding\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]  # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)  # (1, depth)\n",
    "    angle_rads = positions * angle_rates  # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size,\n",
    "                 d_model,\n",
    "                 use_embedding=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_embedding = use_embedding\n",
    "\n",
    "        if self.use_embedding:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Dense(d_model, activation=\"relu\")\n",
    "\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        if self.use_embedding:\n",
    "            return self.embedding.compute_mask(*args, **kwargs)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def call(self, x):\n",
    "        # Ensure x is a 2D tensor: (batch_size, seq_len)\n",
    "        x = self.embedding(x)  # Convert token indices to embeddings\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # Scale embeddings\n",
    "        length = tf.shape(x)[1]  # seq_len\n",
    "        x += self.pos_encoding[tf.newaxis, :length, :]  # Add positional encodings\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "52c8cda8-ca1f-4773-8a22-7c3a5999c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- attention layers\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask=True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttentionFeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1,\n",
    "                 attention=\"global\"):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if attention == \"global\":\n",
    "            self.self_attention = GlobalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        elif attention == \"causal\":\n",
    "            self.self_attention = CausalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        else:\n",
    "            raise NotImplemented(f\"The choice {attention} for attention is not implemented.\")\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttentionCrossAttentionFeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 dropout_rate=0.1,\n",
    "                 attention=\"causal\"):\n",
    "\n",
    "        super(SelfAttentionCrossAttentionFeedForwardLayer, self).__init__()\n",
    "\n",
    "        if attention == \"global\":\n",
    "            self.self_attention = GlobalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        elif attention == \"causal\":\n",
    "            self.self_attention = CausalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        else:\n",
    "            raise NotImplemented(f\"The choice {attention} for attention is not implemented.\")\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x\n",
    "    \n",
    "# --------------------------------------- encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, *, \n",
    "                 num_layers, \n",
    "                 d_model, \n",
    "                 num_heads,\n",
    "                 dff, \n",
    "                 vocab_size, \n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            SelfAttentionFeedForwardLayer(d_model=d_model,\n",
    "                                          num_heads=num_heads,\n",
    "                                          dff=dff,\n",
    "                                          dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "# --------------------------------------- decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            SelfAttentionCrossAttentionFeedForwardLayer(d_model=d_model, num_heads=num_heads,\n",
    "                                                        dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        return x # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "068df023-b466-4960-af54-4819cb6cd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, \n",
    "                 num_layers, \n",
    "                 d_model, \n",
    "                 num_heads, \n",
    "                 dff,\n",
    "                 input_vocab_size, \n",
    "                 target_vocab_size, \n",
    "                 dropout_rate=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, \n",
    "                               d_model=d_model,\n",
    "                               num_heads=num_heads, \n",
    "                               dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               d_model=d_model,\n",
    "                               num_heads=num_heads, \n",
    "                               dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.Sequential([tf.keras.layers.Dense(target_vocab_size)])\n",
    "        \n",
    "    def save_model_weights(self, save_folder):\n",
    "        self.encoder.save_weights(os.path.join(save_folder, \"encoder.weights.h5\"))\n",
    "        self.decoder.save_weights(os.path.join(save_folder, \"decoder.weights.h5\"))\n",
    "        self.final_layer.save_weights(os.path.join(save_folder, \"final_layer.weights.h5\"))\n",
    "\n",
    "    def load_model_from_weights(self, save_folder):\n",
    "        self.encoder.load_weights(os.path.join(save_folder, \"encoder.weights.h5\"))\n",
    "        self.decoder.load_weights(os.path.join(save_folder, \"decoder.weights.h5\"))\n",
    "        self.final_layer.load_weights(os.path.join(save_folder, \"final_layer.weights.h5\"))\n",
    "\n",
    "    def get_models(self):\n",
    "        return self.encoder, self.decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Ensure inputs are indices, not embeddings\n",
    "        encoder_input = inputs\n",
    "        decoder_input = inputs  # Modify this if you use separate inputs for decoder\n",
    "\n",
    "        # Pass token indices (2D) to encoder\n",
    "        encoder_embeddings = self.encoder(encoder_input)  # Shape `(batch_size, seq_len, d_model)`\n",
    "\n",
    "        # Pass encoder output and token indices to decoder\n",
    "        decoder_embeddings = self.decoder(decoder_input, encoder_embeddings)  # Shape `(batch_size, seq_len, d_model)`\n",
    "\n",
    "        # Final dense layer for predictions\n",
    "        logits = self.final_layer(decoder_embeddings)  # Shape `(batch_size, seq_len, vocab_size)`\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08698e-a921-4dec-af54-e2b24ff04a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, transformer, mapper, sample=True, min_prob=0.05):\n",
    "        self.seq_mapper = mapper\n",
    "        self.transformer = transformer\n",
    "        self.sample = sample\n",
    "        assert 0 <= min_prob < 1\n",
    "        self.min_prob = min_prob\n",
    "\n",
    "    def __call__(self, cond_seq, max_length=15): \n",
    "        seq_start = START_TOKEN  # Use the integer directly\n",
    "        seq_end = END_TOKEN      # Use the integer directly\n",
    "\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, seq_start)\n",
    "\n",
    "        # Encoder input is the input sequence for translation\n",
    "        encoder_input = tf.expand_dims(cond_seq, 0)  # Shape: (1, seq_len)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            # Stack the current output and pass it through the model\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer(encoder_input, training=False)\n",
    "\n",
    "            # Get predictions for the next token\n",
    "            predictions = predictions[0, i:i+1, :]  # Shape `(1, vocab_size)`\n",
    "            predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]  # Get the most probable token\n",
    "\n",
    "            output_array = output_array.write(i + 1, predicted_id)\n",
    "\n",
    "            # Stop if END token is predicted\n",
    "            if predicted_id == seq_end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack()).numpy()\n",
    "\n",
    "        # Map integers back to their string equivalents (decode to sourceID)\n",
    "        text = [self.seq_mapper.string_map.get(int(tok), \"UNKNOWN\") for tok in output]\n",
    "\n",
    "        return text, None  # Return text and no attention weights for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90bf018-5c36-42cc-8c72-e157f5dae1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- main use case\n",
    "def main(train=True, \n",
    "         compute_results=True,\n",
    "         n_conds=-1,\n",
    "         n_samples=3,\n",
    "         epochs=1, \n",
    "         checkpoint_name=\"test_transformer\"):\n",
    "    \n",
    "    # Path to tokenization directory\n",
    "    tokenization_dir = \"../data/Tokenization\"  # Directory containing the 300 CSV files\n",
    "    csv_files = sorted(\n",
    "        [os.path.join(tokenization_dir, file) for file in os.listdir(tokenization_dir) if file.endswith(\".csv\")]\n",
    "    )\n",
    "    \n",
    "    # Extract `sourceID` sequences from the CSV files\n",
    "    sequences = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        data = pd.read_csv(file)\n",
    "        source_ids = data['sourceID'].dropna().astype(int).tolist()\n",
    "        if source_ids:\n",
    "            sequences.append(source_ids)     # Use the entire column as the sequence\n",
    "    \n",
    "    # Save raw data for reference\n",
    "    os.makedirs(checkpoint_name, exist_ok=True)\n",
    "    with open(os.path.join(checkpoint_name, \"raw_data.txt\"), \"w\") as f:\n",
    "        print_dict = {}\n",
    "    \n",
    "    # Add start and end tokens\n",
    "    sequences = add_start_end_tokens(sequences)\n",
    "    sequences = make_same_length(sequences)\n",
    "    \n",
    "    # Save processed data for training\n",
    "    with open(os.path.join(checkpoint_name, \"train_data.txt\"), \"w\") as f:\n",
    "        print_dict = {}\n",
    "        for i, seq in enumerate(sequences):\n",
    "            print_dict[i] = seq\n",
    "                \n",
    "        with redirect_stdout(f):\n",
    "            pprint(print_dict)\n",
    "    \n",
    "    # Instantiate mappers for conditions and sequences\n",
    "    seq_mapper = SeqMapper()\n",
    "\n",
    "    \n",
    "    # generate training data\n",
    "    model_input_data, model_output_data = make_training_data(sequences)\n",
    "    model_input_data = tf.convert_to_tensor(model_input_data, dtype=tf.int32)\n",
    "    model_output_data = tf.convert_to_tensor(model_output_data, dtype=tf.int32)\n",
    "    print(f\"Max value in input data: {np.max(model_input_data.numpy())}\")\n",
    "    print(f\"Min value in input data: {np.min(model_input_data.numpy())}\")\n",
    "    print(f\"Expected vocab_size: {seq_mapper.dimension}\")\n",
    "    print(f\"Shape of model_input_data: {model_input_data.shape}\")\n",
    "\n",
    "    # build transformer\n",
    "    \n",
    "    # tf example\n",
    "    # num_layers = 4\n",
    "    # d_model = 128\n",
    "    # dff = 512\n",
    "    # num_heads = 8\n",
    "    # dropout_rate = 0.1\n",
    "    \n",
    "    transformer = Transformer(\n",
    "        num_layers=3,\n",
    "        d_model=32,\n",
    "        num_heads=8,\n",
    "        dff=128,\n",
    "        input_vocab_size=seq_mapper.dimension,\n",
    "        target_vocab_size=seq_mapper.dimension,\n",
    "        dropout_rate=0.1,\n",
    "    )\n",
    "    \n",
    "    transformer(model_input_data)\n",
    "    transformer.summary()\n",
    "    \n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        os.mkdir(checkpoint_name)\n",
    "    \n",
    "    if os.path.exists(os.path.join(checkpoint_name, \"encoder.weights.h5\")):\n",
    "        transformer.load_model_from_weights(checkpoint_name)\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # compile transformer\n",
    "    transformer.compile(\n",
    "        loss=masked_loss,\n",
    "        optimizer='Adam',\n",
    "        metrics=[masked_accuracy])\n",
    "    \n",
    "    if train:\n",
    "        print(\"---------------------------- Training model ----------------------------\")\n",
    "        \n",
    "        # fit transformer on batches\n",
    "        transformer.fit(model_input_data,\n",
    "                        model_output_data,\n",
    "                        epochs=epochs,\n",
    "                        # validation_data=val_batches\n",
    "                        )\n",
    "        \n",
    "        transformer.save_model_weights(checkpoint_name)\n",
    "    \n",
    "    # build translator\n",
    "    translator = Translator(transformer, mapper=(seq_mapper))\n",
    "    \n",
    "    # translate examples\n",
    "    \n",
    "    if compute_results:\n",
    "        print(\"---------------------------- Using model ----------------------------\")\n",
    "        \n",
    "        translation_results = dict()\n",
    "    \n",
    "        for idx, seq in enumerate(sequences[:n_samples]):\n",
    "            print(f\"Generating output for sequence {idx}\")\n",
    "            model_text, _ = translator(seq)  # Get the decoded output\n",
    "            translation_results[idx] = \" \".join(model_text)  # Save as a readable string\n",
    "\n",
    "    # Save results to a file\n",
    "        with open(os.path.join(checkpoint_name, \"results.txt\"), \"w\") as f:\n",
    "            with redirect_stdout(f):\n",
    "                pprint(translation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "3e468962-6017-4a1f-9ee3-646f67575dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value in input data: 14\n",
      "Min value in input data: 1\n",
      "Expected vocab_size: 15\n",
      "Shape of model_input_data: (326, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'global_self_attention_21' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_49' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_42' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_21' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'global_self_attention_22' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_50' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_43' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_22' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'global_self_attention_23' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_51' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_44' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_23' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_21' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'cross_attention_21' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_52' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_45' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_cross_attention_feed_forward_layer_21' (of type SelfAttentionCrossAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_22' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'cross_attention_22' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_53' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_46' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_cross_attention_feed_forward_layer_22' (of type SelfAttentionCrossAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_23' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'cross_attention_23' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_54' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_47' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_cross_attention_feed_forward_layer_23' (of type SelfAttentionCrossAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_55' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">126,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)            │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">227,520</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)      │ (326, 39, 15)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">495</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_7 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m126,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_15 (\u001b[38;5;33mDecoder\u001b[0m)            │ ?                      │       \u001b[38;5;34m227,520\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_55 (\u001b[38;5;33mSequential\u001b[0m)      │ (326, 39, 15)          │           \u001b[38;5;34m495\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">354,639</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m354,639\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">354,639</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m354,639\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "\n",
      "---------------------------- Training model ----------------------------\n",
      "Epoch 1/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - loss: 0.2590 - masked_accuracy: 0.9106\n",
      "Epoch 2/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.2118 - masked_accuracy: 0.9293\n",
      "Epoch 3/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.1972 - masked_accuracy: 0.9341\n",
      "Epoch 4/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.1821 - masked_accuracy: 0.9386\n",
      "Epoch 5/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.1829 - masked_accuracy: 0.9373\n",
      "---------------------------- Using model ----------------------------\n",
      "Generating output for condition A\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition B\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition C\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition D\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition E\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition F\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition G\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition H\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition I\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition J\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition K\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition L\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition M\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition N\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition O\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition P\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition Q\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition R\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition S\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition T\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition U\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition V\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition W\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition X\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition Y\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for condition Z\n",
      "Generating output for sequence 0\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 1\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 2\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 3\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 4\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 5\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 6\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 7\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 8\n",
      "Shape of input to Transformer: (1, 1)\n",
      "Generating output for sequence 9\n",
      "Shape of input to Transformer: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(train=True, \n",
    "         compute_results=True, \n",
    "         n_conds=-1,\n",
    "         n_samples=10,\n",
    "         epochs=5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
