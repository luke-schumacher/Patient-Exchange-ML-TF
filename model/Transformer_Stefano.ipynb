{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e91a6689-efe7-41ba-9ee2-afa668d26754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os: Built-in module (no version)\n",
      "pprint: Built-in module (no version)\n",
      "contextlib: Built-in module (no version)\n",
      "numpy version: 1.26.4\n",
      "tensorflow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "from contextlib import redirect_stdout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Output library versions\n",
    "print(f\"os: Built-in module (no version)\")\n",
    "print(f\"pprint: Built-in module (no version)\")\n",
    "print(f\"contextlib: Built-in module (no version)\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bb1b001d-c4b2-4ec0-af1c-e060788fe8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated encoding legend\n",
    "ENCODING_LEGEND = {\n",
    "    'MRI_CCS_11': 1, 'MRI_EXU_95': 2, 'MRI_FRR_18': 3, 'MRI_FRR_257': 4,\n",
    "    'MRI_FRR_264': 5, 'MRI_FRR_3': 6, 'MRI_FRR_34': 7, 'MRI_MPT_1005': 8,\n",
    "    'MRI_MSR_100': 9, 'MRI_MSR_104': 10, 'MRI_MSR_21': 11, 'MRI_MSR_34': 12,\n",
    "    'START': 13,  # Start token\n",
    "    'END': 14     # End token\n",
    "}\n",
    "\n",
    "CHAR_TO_INT = {\n",
    "    '0': 0,\n",
    "    '1': 1,\n",
    "    '2': 2,\n",
    "    '3': 3,\n",
    "    '4': 4,\n",
    "    '5': 5,\n",
    "    '6': 6,\n",
    "    '7': 7,\n",
    "    '8': 8,\n",
    "    '9': 9,\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f36d62a4-3ba4-47cf-a556-24b4829ab219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- DATA FUNCTIONS -----------------------------------\n",
    "\n",
    "# Updated start and end tokens\n",
    "START_TOKEN = 13\n",
    "END_TOKEN = 14\n",
    "\n",
    "def generate_data(data_size=100):\n",
    "    \"\"\"\n",
    "    Generate synthetic sourceID data sequences.\n",
    "    Each sequence starts with the START token (13) and ends with the END token (14).\n",
    "    Random sourceIDs (from 1 to 12) are included in between.\n",
    "    \"\"\"\n",
    "    \n",
    "    def generate_cond_sequence(condition):\n",
    "        \"\"\"\n",
    "        Generate a sequence of sourceIDs based on a condition.\n",
    "        The condition is an integer (sourceID) between 1 and 12.\n",
    "        \"\"\"\n",
    "        if condition < 1 or condition > 12:\n",
    "            raise ValueError(\"Condition must be between 1 and 12.\")\n",
    "\n",
    "        random_number = np.random.choice([0, 1, 2])\n",
    "\n",
    "        if random_number == 0:\n",
    "            length = 3\n",
    "        elif random_number == 1:\n",
    "            length = 6\n",
    "        elif random_number == 2:\n",
    "            length = 10\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # Generate random sourceIDs for the sequence\n",
    "        return np.random.randint(1, 13, size=length).tolist()\n",
    "\n",
    "    conditions_list = []\n",
    "    sequences_lists = []\n",
    "\n",
    "    for i in range(data_size):\n",
    "        # Randomly select a condition (sourceID between 1 and 12)\n",
    "        condition = np.random.randint(1, 13)\n",
    "\n",
    "        # Generate a sequence based on the condition\n",
    "        seq = generate_cond_sequence(condition)\n",
    "\n",
    "        # Add start and end tokens to the sequence\n",
    "        seq = [START_TOKEN] + seq + [END_TOKEN]\n",
    "\n",
    "        conditions_list.append(condition)\n",
    "        sequences_lists.append(seq)\n",
    "\n",
    "    return conditions_list, sequences_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2600d879-cca7-4d62-8ea6-ffc729fa0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_tokens(conds, seqs):\n",
    "    new_seqs = []\n",
    "    for seq in seqs:\n",
    "        # Prepend START token and append END token\n",
    "        new_seqs.append([13] + seq + [14])  # Use integers for START and END tokens\n",
    "    return conds, new_seqs\n",
    "    \n",
    "    \n",
    "def make_same_length(conds, seqs):\n",
    "    new_seqs = []\n",
    "    \n",
    "    # Find the maximum sequence length\n",
    "    max_length = max(len(seq) for seq in seqs)\n",
    "    \n",
    "    for seq in seqs:\n",
    "        # Calculate how many tokens to add\n",
    "        remaining_tokens = max_length - len(seq)\n",
    "        # Pad the sequence with the END token (14 as per your legend)\n",
    "        new_seqs.append(seq + [14] * remaining_tokens)  # END_TOKEN as an integer\n",
    "    \n",
    "    return conds, new_seqs\n",
    "\n",
    "\n",
    "\n",
    "def make_training_data(conds, seqs):\n",
    "    \n",
    "    input_seqs = []\n",
    "    \n",
    "    for seq in seqs:\n",
    "        input_seqs.append(seq[:-1])\n",
    "    \n",
    "    input_data = (conds, input_seqs)\n",
    "    \n",
    "    output_seqs = []\n",
    "    \n",
    "    for seq in seqs:\n",
    "        output_seqs.append(seq[1:])\n",
    "        \n",
    "    output_data = output_seqs\n",
    "    \n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "464e3a43-9fba-48b7-b886-90eb9d2291bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n"
     ]
    }
   ],
   "source": [
    "# Define mappings\n",
    "RAW_STRINGS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "CHAR_TO_INT = dict(zip(RAW_STRINGS, range(1, len(RAW_STRINGS) + 1)))\n",
    "INT_TO_CHAR = {v: k for k, v in CHAR_TO_INT.items()}\n",
    "\n",
    "class ConditionMapper:\n",
    "    def __init__(self):\n",
    "        # Define your mappings as before\n",
    "        self.integer_map = ENCODING_LEGEND.copy()  # Your original map\n",
    "        self.string_map = {v: k for k, v in self.integer_map.items()}  # Reverse mapping for integer to string\n",
    "        self.dimension = len(self.integer_map) + 1  # Account for possible 'END' token\n",
    "\n",
    "    def map_to_ints(self, input_string):\n",
    "        # Check if the input is a string (e.g., 'MRI_MSR_100') or an integer (e.g., 10)\n",
    "        if isinstance(input_string, int):\n",
    "            # Check if the integer exists in the reverse mapping (for keys like 10 corresponding to 'MRI_MSR_100')\n",
    "            input_string = self.string_map.get(input_string, None)\n",
    "            if input_string is None:\n",
    "                raise KeyError(f\"Integer '{input_string}' not found in string map\")\n",
    "        \n",
    "        # Now input_string should be a valid string that can be mapped to an integer\n",
    "        if input_string not in self.integer_map:\n",
    "            raise KeyError(f\"Character '{input_string}' not found in integer map\")\n",
    "        \n",
    "        return np.array([self.integer_map[input_string]])\n",
    "\n",
    "    def map_list_to_ints_vectors(self, list_of_strings):\n",
    "        return np.array([self.map_to_ints(input_string) for input_string in list_of_strings])\n",
    "\n",
    "    def map_ints_to_string(self, input_ints):\n",
    "        return \"\".join([self.string_map.get(integer, '') for integer in input_ints])\n",
    "\n",
    "# Example usage:\n",
    "cond_mapper = ConditionMapper()\n",
    "\n",
    "# If you're working with integers:\n",
    "input_string = 10  # This should map to a valid string if it corresponds to a known integer\n",
    "output_int = cond_mapper.map_to_ints(input_string)\n",
    "print(output_int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "274cf379-1295-4e82-81d5-bdafdb70b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqMapper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.integer_map = CHAR_TO_INT.copy()\n",
    "        \n",
    "        n_ints = len(self.integer_map)\n",
    "        \n",
    "        # self.integer_map.update({START_TOKEN: n_ints, END_TOKEN: n_ints + 1})\n",
    "        # self.dimension = len(self.integer_map)\n",
    "        \n",
    "        self.integer_map.update({START_TOKEN: n_ints + 1, END_TOKEN: n_ints + 2})\n",
    "        \n",
    "        self.dimension = len(self.integer_map) + 1\n",
    "        \n",
    "        self.string_map = dict(zip(list(self.integer_map.values()), list(self.integer_map.keys())))\n",
    "    \n",
    "    def map_to_ints(self, input_string):\n",
    "        input_numbers = [self.integer_map[item] for item in input_string]\n",
    "        \n",
    "        return np.asarray(input_numbers)\n",
    "    \n",
    "    def map_list_to_ints_vectors(self, list_of_strings):\n",
    "        input_length = len(list_of_strings)\n",
    "        \n",
    "        vectors = []\n",
    "        \n",
    "        for input_string in list_of_strings:\n",
    "            vectors.append(self.map_to_ints(input_string))\n",
    "            \n",
    "        vectors = np.asarray(vectors)\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    def map_ints_to_string(self, input_ints):\n",
    "        string = \"\"\n",
    "        \n",
    "        for integer in input_ints:\n",
    "            string += self.string_map[integer]\n",
    "            \n",
    "        return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bf9eaa06-b19f-4e12-807c-2af1f9c35ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_training_data(input_train, output_train, mappers):\n",
    "    converted_input_train = mappers[0].map_list_to_ints_vectors(input_train[0]), mappers[1].map_list_to_ints_vectors(input_train[1])\n",
    "    converted_output_train = mappers[1].map_list_to_ints_vectors(output_train)\n",
    "    \n",
    "    return converted_input_train, converted_output_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d26f0f4a-0b95-42f3-89cd-50b4d6fdfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- TENSORFLOW LAYERS AND MODELS -----------------------------------\n",
    "    \n",
    "# --------------------------------------- masked layers\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7d18f78f-f364-4b09-a469-eb94a19f93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- positional embedding\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]  # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)  # (1, depth)\n",
    "    angle_rads = positions * angle_rates  # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size,\n",
    "                 d_model,\n",
    "                 use_embedding=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_embedding = use_embedding\n",
    "\n",
    "        if self.use_embedding:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Dense(d_model, activation=\"relu\")\n",
    "\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        if self.use_embedding:\n",
    "            return self.embedding.compute_mask(*args, **kwargs)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positional_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "52c8cda8-ca1f-4773-8a22-7c3a5999c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- attention layers\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask=True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttentionFeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1,\n",
    "                 attention=\"global\"):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if attention == \"global\":\n",
    "            self.self_attention = GlobalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        elif attention == \"causal\":\n",
    "            self.self_attention = CausalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        else:\n",
    "            raise NotImplemented(f\"The choice {attention} for attention is not implemented.\")\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttentionCrossAttentionFeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 dropout_rate=0.1,\n",
    "                 attention=\"causal\"):\n",
    "\n",
    "        super(SelfAttentionCrossAttentionFeedForwardLayer, self).__init__()\n",
    "\n",
    "        if attention == \"global\":\n",
    "            self.self_attention = GlobalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        elif attention == \"causal\":\n",
    "            self.self_attention = CausalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "        else:\n",
    "            raise NotImplemented(f\"The choice {attention} for attention is not implemented.\")\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x\n",
    "    \n",
    "# --------------------------------------- encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, *, \n",
    "                 num_layers, \n",
    "                 d_model, \n",
    "                 num_heads,\n",
    "                 dff, \n",
    "                 vocab_size, \n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            SelfAttentionFeedForwardLayer(d_model=d_model,\n",
    "                                          num_heads=num_heads,\n",
    "                                          dff=dff,\n",
    "                                          dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "# --------------------------------------- decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            SelfAttentionCrossAttentionFeedForwardLayer(d_model=d_model, num_heads=num_heads,\n",
    "                                                        dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        return x # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "068df023-b466-4960-af54-4819cb6cd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, \n",
    "                 num_layers, \n",
    "                 d_model, \n",
    "                 num_heads, \n",
    "                 dff,\n",
    "                 input_vocab_size, \n",
    "                 target_vocab_size, \n",
    "                 dropout_rate=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, \n",
    "                               d_model=d_model,\n",
    "                               num_heads=num_heads, \n",
    "                               dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               d_model=d_model,\n",
    "                               num_heads=num_heads, \n",
    "                               dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.Sequential([tf.keras.layers.Dense(target_vocab_size)])\n",
    "        \n",
    "    def save_model_weights(self, save_folder):\n",
    "        self.encoder.save_weights(os.path.join(save_folder, \"encoder.weights.h5\"))\n",
    "        self.decoder.save_weights(os.path.join(save_folder, \"decoder.weights.h5\"))\n",
    "        self.final_layer.save_weights(os.path.join(save_folder, \"final_layer.weights.h5\"))\n",
    "\n",
    "    def load_model_from_weights(self, save_folder):\n",
    "        self.encoder.load_weights(os.path.join(save_folder, \"encoder.weights.h5\"))\n",
    "        self.decoder.load_weights(os.path.join(save_folder, \"decoder.weights.h5\"))\n",
    "        self.final_layer.load_weights(os.path.join(save_folder, \"final_layer.weights.h5\"))\n",
    "\n",
    "    def get_models(self):\n",
    "        return self.encoder, self.decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, x = inputs\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "cb08698e-a921-4dec-af54-e2b24ff04a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- translator\n",
    "class Translator(tf.Module):\n",
    "    def __init__(self, transformer, mappers, sample=True, min_prob=0.05):\n",
    "        self.cond_mapper = mappers[0]\n",
    "        self.seq_mapper = mappers[1]\n",
    "        self.transformer = transformer\n",
    "        \n",
    "        self.sample = sample\n",
    "        \n",
    "        assert 0 <= min_prob < 1\n",
    "        \n",
    "        self.min_prob = min_prob\n",
    "\n",
    "    def __call__(self, cond_seq, max_length=15):\n",
    "        if not isinstance(cond_seq, tf.Tensor):\n",
    "            cond_seq = tf.convert_to_tensor(cond_seq)\n",
    "\n",
    "        encoder_input = tf.expand_dims(cond_seq, 0)\n",
    "        \n",
    "        seq_start = self.seq_mapper.integer_map[START_TOKEN]\n",
    "        seq_end = self.seq_mapper.integer_map[END_TOKEN]\n",
    "\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, seq_start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer([encoder_input, tf.expand_dims(output, 0)], training=False)\n",
    "\n",
    "            predictions = predictions[0, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "            \n",
    "            if self.sample:\n",
    "                # calculate and transform probabilities\n",
    "                probabilities = tf.math.softmax(predictions)\n",
    "                high_probs = tf.where(probabilities > self.min_prob, probabilities, 0)[0]\n",
    "                high_probs = high_probs / tf.math.reduce_sum(high_probs)\n",
    "                \n",
    "                # print(high_probs)\n",
    "                \n",
    "                # sample next id\n",
    "                predicted_id = np.random.choice(len(high_probs), size=1, p=high_probs.numpy())[0]\n",
    "            else:\n",
    "                # take id with highest probability\n",
    "                predicted_id = tf.argmax(predictions, axis=-1)[0]\n",
    "\n",
    "            output_array = output_array.write(i + 1, predicted_id)\n",
    "\n",
    "            if predicted_id == seq_end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "\n",
    "        text = self.seq_mapper.map_ints_to_string(output.numpy())  # Shape: `()`.\n",
    "\n",
    "        self.transformer([encoder_input, tf.expand_dims(output[:-1], 0)], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d90bf018-5c36-42cc-8c72-e157f5dae1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- main use case\n",
    "def main(train=True, \n",
    "         compute_results=True,\n",
    "         n_conds=-1,\n",
    "         n_samples=3,\n",
    "         epochs=1, \n",
    "         checkpoint_name=\"test_transformer\"):\n",
    "    \n",
    "    # Path to tokenization directory\n",
    "    tokenization_dir = \"../data/Tokenization\"  # Directory containing the 300 CSV files\n",
    "    csv_files = sorted(\n",
    "        [os.path.join(tokenization_dir, file) for file in os.listdir(tokenization_dir) if file.endswith(\".csv\")]\n",
    "    )\n",
    "    \n",
    "    # Extract `sourceID` sequences from the CSV files\n",
    "    conditions = []\n",
    "    sequences = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        data = pd.read_csv(file)  # Load CSV file\n",
    "        source_ids = data['sourceID'].dropna().astype(int).tolist()  # Extract `sourceID` column, drop NaNs, convert to int\n",
    "        \n",
    "        if source_ids:\n",
    "            conditions.append(source_ids[0])  # Use the first `sourceID` as the condition\n",
    "            sequences.append(source_ids)     # Use the entire column as the sequence\n",
    "    \n",
    "    # Save raw data for reference\n",
    "    os.makedirs(checkpoint_name, exist_ok=True)\n",
    "    with open(os.path.join(checkpoint_name, \"raw_data.txt\"), \"w\") as f:\n",
    "        print_dict = {}\n",
    "        for i, cond in enumerate(conditions):\n",
    "            if cond not in print_dict:\n",
    "                print_dict[cond] = []\n",
    "            \n",
    "            print_dict[cond].append(sequences[i])\n",
    "                \n",
    "        with redirect_stdout(f):\n",
    "            pprint(print_dict)\n",
    "    \n",
    "    # Add start and end tokens to sequences\n",
    "    conditions, sequences = add_start_end_tokens(conditions, sequences)\n",
    "    conditions, sequences = make_same_length(conditions, sequences)\n",
    "    \n",
    "    # Save processed data for training\n",
    "    with open(os.path.join(checkpoint_name, \"train_data.txt\"), \"w\") as f:\n",
    "        print_dict = {}\n",
    "        for i, cond in enumerate(conditions):\n",
    "            if cond not in print_dict:\n",
    "                print_dict[cond] = []\n",
    "            \n",
    "            print_dict[cond].append(sequences[i])\n",
    "                \n",
    "        with redirect_stdout(f):\n",
    "            pprint(print_dict)\n",
    "    \n",
    "    # Instantiate mappers for conditions and sequences\n",
    "    cond_mapper = ConditionMapper()\n",
    "    seq_mapper = SeqMapper()\n",
    "\n",
    "    \n",
    "    # generate training data\n",
    "    raw_model_input_data, raw_model_output_data = make_training_data(conditions, sequences)\n",
    "    model_input_data, model_output_data = convert_training_data(raw_model_input_data, \n",
    "                                                                raw_model_output_data, \n",
    "                                                                mappers=(cond_mapper, seq_mapper))\n",
    "    \n",
    "    # build transformer\n",
    "    \n",
    "    # tf example\n",
    "    # num_layers = 4\n",
    "    # d_model = 128\n",
    "    # dff = 512\n",
    "    # num_heads = 8\n",
    "    # dropout_rate = 0.1\n",
    "    \n",
    "    transformer = Transformer(\n",
    "        num_layers=3,\n",
    "        d_model=32,\n",
    "        num_heads=8,\n",
    "        dff=128,\n",
    "        input_vocab_size=cond_mapper.dimension,\n",
    "        target_vocab_size=seq_mapper.dimension,\n",
    "        dropout_rate=0.1,\n",
    "    )\n",
    "    \n",
    "    transformer(model_input_data)\n",
    "    transformer.summary()\n",
    "    \n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        os.mkdir(checkpoint_name)\n",
    "    \n",
    "    if os.path.exists(os.path.join(checkpoint_name, \"encoder.weights.h5\")):\n",
    "        transformer.load_model_from_weights(checkpoint_name)\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # compile transformer\n",
    "    transformer.compile(\n",
    "        loss=masked_loss,\n",
    "        optimizer='Adam',\n",
    "        metrics=[masked_accuracy])\n",
    "    \n",
    "    if train:\n",
    "        print(\"---------------------------- Training model ----------------------------\")\n",
    "        \n",
    "        # fit transformer on batches\n",
    "        transformer.fit(model_input_data,\n",
    "                        model_output_data,\n",
    "                        epochs=epochs,\n",
    "                        # validation_data=val_batches\n",
    "                        )\n",
    "        \n",
    "        transformer.save_model_weights(checkpoint_name)\n",
    "    \n",
    "    # build translator\n",
    "    translator = Translator(transformer, mappers=(cond_mapper, seq_mapper))\n",
    "    \n",
    "    # translate examples\n",
    "    \n",
    "    if compute_results:\n",
    "        print(\"---------------------------- Using model ----------------------------\")\n",
    "            \n",
    "        translation_results = dict()\n",
    "        \n",
    "        if n_conds == -1:\n",
    "            strings_to_use = RAW_STRINGS\n",
    "        elif 0 < n_conds < len(RAW_STRINGS):\n",
    "            strings_to_use = RAW_STRINGS[:n_conds]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        for char in RAW_STRINGS:\n",
    "            \n",
    "            print(f\"Generating output for condition {char}\")\n",
    "            \n",
    "            translation_results[char] = []\n",
    "            example_cond = cond_mapper.map_to_ints(char)\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                model_text, _ = translator(example_cond)\n",
    "                \n",
    "                print(model_text)\n",
    "                \n",
    "                translation_results[char].append(model_text)\n",
    "        \n",
    "        with open(os.path.join(checkpoint_name, \"results.txt\"), \"w\") as f:\n",
    "            with redirect_stdout(f):\n",
    "                pprint(translation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3e468962-6017-4a1f-9ee3-646f67575dce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [208]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcompute_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[43mn_conds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m         \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [207]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(train, compute_results, n_conds, n_samples, epochs, checkpoint_name)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# generate training data\u001b[39;00m\n\u001b[0;32m     62\u001b[0m raw_model_input_data, raw_model_output_data \u001b[38;5;241m=\u001b[39m make_training_data(conditions, sequences)\n\u001b[1;32m---> 63\u001b[0m model_input_data, model_output_data \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_model_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mraw_model_output_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mmappers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcond_mapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_mapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# build transformer\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# tf example\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# num_heads = 8\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# dropout_rate = 0.1\u001b[39;00m\n\u001b[0;32m     76\u001b[0m transformer \u001b[38;5;241m=\u001b[39m Transformer(\n\u001b[0;32m     77\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     78\u001b[0m     d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     84\u001b[0m )\n",
      "Input \u001b[1;32mIn [201]\u001b[0m, in \u001b[0;36mconvert_training_data\u001b[1;34m(input_train, output_train, mappers)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_training_data\u001b[39m(input_train, output_train, mappers):\n\u001b[1;32m----> 2\u001b[0m     converted_input_train \u001b[38;5;241m=\u001b[39m mappers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmap_list_to_ints_vectors(input_train[\u001b[38;5;241m0\u001b[39m]), \u001b[43mmappers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_list_to_ints_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     converted_output_train \u001b[38;5;241m=\u001b[39m mappers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmap_list_to_ints_vectors(output_train)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converted_input_train, converted_output_train\n",
      "Input \u001b[1;32mIn [200]\u001b[0m, in \u001b[0;36mSeqMapper.map_list_to_ints_vectors\u001b[1;34m(self, list_of_strings)\u001b[0m\n\u001b[0;32m     25\u001b[0m vectors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_string \u001b[38;5;129;01min\u001b[39;00m list_of_strings:\n\u001b[1;32m---> 28\u001b[0m     vectors\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_to_ints\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_string\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m vectors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(vectors)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vectors\n",
      "Input \u001b[1;32mIn [200]\u001b[0m, in \u001b[0;36mSeqMapper.map_to_ints\u001b[1;34m(self, input_string)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_to_ints\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_string):\n\u001b[1;32m---> 18\u001b[0m     input_numbers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteger_map[item] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m input_string]\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(input_numbers)\n",
      "Input \u001b[1;32mIn [200]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_to_ints\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_string):\n\u001b[1;32m---> 18\u001b[0m     input_numbers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteger_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m input_string]\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(input_numbers)\n",
      "\u001b[1;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(train=True, \n",
    "         compute_results=True, \n",
    "         n_conds=-1,\n",
    "         n_samples=10,\n",
    "         epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
