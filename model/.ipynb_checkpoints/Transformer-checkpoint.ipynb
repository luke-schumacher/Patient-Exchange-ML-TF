{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a52984ef-399a-482a-9f4a-aa1f8b9b0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Embedding, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15304c14-cf87-4a5d-8c71-22d0fb9ec508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "learning_rate = 0.01\n",
    "key_dim = 16\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "83f61e6d-43e0-412a-854a-fa1c92840451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from ../data/encoded_176398_HEAD.csv.\n",
      "\n",
      "First 5 rows of the dataset before any processing:\n",
      "              datetime  sourceID  timediff  ZAxisInPossible  ZAxisOutPossible  \\\n",
      "0  2023-03-27 08:14:34        10       0.0              NaN               NaN   \n",
      "1  2023-03-27 08:14:49         4      15.0              NaN               NaN   \n",
      "2  2023-03-27 08:14:56         5      22.0              1.0               0.0   \n",
      "3  2023-03-27 08:15:08         1      34.0              1.0               0.0   \n",
      "4  2023-03-27 08:15:39        12      65.0              1.0               0.0   \n",
      "\n",
      "   YAxisDownPossible  YAxisUpPossible       PTAB  BC  S1  ...  C24  EN  SHL  \\\n",
      "0                NaN              NaN        NaN   0 NaN  ...  NaN NaN  NaN   \n",
      "1                NaN              NaN -1127700.0   0 NaN  ...  NaN NaN  NaN   \n",
      "2                1.0              1.0 -1127700.0   0 NaN  ...  NaN NaN  NaN   \n",
      "3                1.0              1.0 -1127700.0   1 NaN  ...  NaN NaN  NaN   \n",
      "4                1.0              1.0 -1127700.0   1 NaN  ...  NaN NaN  NaN   \n",
      "\n",
      "   SHS  BodyPart_from  BodyPart_to                            PatientID_from  \\\n",
      "0  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "1  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "2  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "3  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "4  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "\n",
      "                               PatientID_to  BodyGroup_from  BodyGroup_to  \n",
      "0  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "1  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "2  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "3  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "4  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "\n",
      "[5 rows x 122 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the already encoded data\n",
    "file_path = \"../data/encoded_176398_HEAD.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Data loaded successfully from {file_path}.\\n\")\n",
    "print(\"First 5 rows of the dataset before any processing:\")\n",
    "print(df.head())  # Print first few rows to understand the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf2dad6e-dc1d-44a1-85c6-6f9f54471a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped unnecessary columns.\n",
      "Remaining columns: ['sourceID', 'timediff', 'PTAB', 'BodyGroup_from', 'BodyGroup_to']\n",
      "   sourceID  timediff       PTAB  BodyGroup_from  BodyGroup_to\n",
      "0        10       0.0        NaN               1             4\n",
      "1         4      15.0 -1127700.0               1             4\n",
      "2         5      22.0 -1127700.0               1             4\n",
      "3         1      34.0 -1127700.0               1             4\n",
      "4        12      65.0 -1127700.0               1             4\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns (based on your latest specification)\n",
    "columns_to_drop = ['datetime', 'SN', 'ZAxisInPossible', 'ZAxisOutPossible', 'YAxisDownPossible', \n",
    "                   'YAxisUpPossible', 'BC', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', \n",
    "                   'S5', 'S6', 'S7', 'S8', 'S9', 'BO1', 'BO2', 'BO3', 'B1', 'B2', 'B3', 'B4', \n",
    "                   'B5', 'HE2', 'HE4', 'NE2', 'HE1', 'HE3', 'NE1', 'SHA', 'HW1', 'HW2', 'HW3', \n",
    "                   '18K', 'FA', 'TO', 'BAL', 'BAR', 'BCL', 'BCR', 'HC2', 'HC4', 'HC6', 'HC7', \n",
    "                   'NC2', 'HC1', 'HC3', 'HC5', 'NC1', 'Na', 'UFL', 'PA1', 'PA2', 'PA3', 'PA4', \n",
    "                   'PA5', 'PA6', 'SP1', 'SP2', 'SP3', 'SP4', 'SP5', 'SP6', 'SP7', 'SP8', 'BL8', \n",
    "                   'BR8', 'UFS', 'HEA', 'HEP', 'SC', 'PeH', 'PeN', 'FS', 'FL', 'BY1', 'BY2', \n",
    "                   'BY3', 'BL', 'BR', 'HE', 'BL4', 'BR4', 'BL1', 'BR1', 'BL2', 'BR2', 'L7', \n",
    "                   'L4', 'H2L', 'N2L', 'H1U', 'N1U', 'He1', 'He2', 'TR1', 'TR2', 'TR3', 'TR4', \n",
    "                   'TR5', 'TR6', 'MR', 'ML', 'BL5', 'BR5', 'C24', 'EN', 'SHL', 'SHS','BodyPart_from', \n",
    "                   'BodyPart_to', 'PatientID_from', 'PatientID_to']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "print(\"Dropped unnecessary columns.\")\n",
    "print(\"Remaining columns:\", df.columns.tolist())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f1397fae-e907-4020-b74b-3d16be7a0707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sourceID  timediff       PTAB  BodyGroup_from  BodyGroup_to\n",
      "0        10       0.0        0.0               1             4\n",
      "1         4      15.0 -1127700.0               1             4\n",
      "2         5      22.0 -1127700.0               1             4\n",
      "3         1      34.0 -1127700.0               1             4\n",
      "4        12      65.0 -1127700.0               1             4\n"
     ]
    }
   ],
   "source": [
    "# Replace NaN values with 0 in the DataFrame\n",
    "df.fillna(0, inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1904656c-1090-4e8a-97d3-8f53cb5a1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target (sourceID)\n",
    "X = df.drop(columns=['sourceID'])\n",
    "y_sourceid = df['sourceID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "78bdf0b2-71d9-4890-bdc5-603a45a1c2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few one-hot encoded 'sourceID' values:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Shape of one-hot encoded 'sourceID': (4501, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode 'sourceID'\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_sourceid_encoded = encoder.fit_transform(y_sourceid.values.reshape(-1, 1))\n",
    "original_sourceids = encoder.categories_[0]\n",
    "# Print the first few rows of y_sourceid_encoded to confirm one-hot encoding worked\n",
    "print(\"\\nFirst few one-hot encoded 'sourceID' values:\")\n",
    "print(y_sourceid_encoded[:5])\n",
    "print(\"Shape of one-hot encoded 'sourceID':\", y_sourceid_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ce7e72e-eb07-4a28-8e1d-f55f0755d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ff40f546-c53a-4ede-882c-6a475efc375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sequence length of 5 for Transformer input\n",
    "sequence_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "655d282c-1adf-424f-8888-2f2032944cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences created with start and stop tokens.\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, target, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        targets.append(target[i+seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "X_sequences, y_sequences = create_sequences(X_scaled, y_sourceid_encoded, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6f863f69-8264-4adc-834e-00ba1562ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder model\n",
    "def transformer_model(input_shape, output_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Embedding layer to transform input to higher dimensional space (if needed)\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    \n",
    "    # Multi-head attention layer\n",
    "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    \n",
    "    # Add & Norm\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "    \n",
    "    # Feedforward layer\n",
    "    ff_dim = 256  # adjust this dimension as needed\n",
    "    x_ffn = Dense(ff_dim, activation='relu')(x)\n",
    "    x_ffn = Dense(x.shape[-1])(x_ffn) \n",
    "    \n",
    "    # Final classification layer (softmax for multi-class)\n",
    "    outputs = Dense(output_dim, activation='softmax')(x[:, -1, :])  # Only last timestep output\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "108f88fd-26fb-4358-bf34-fdfa2b755cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "input_shape = (sequence_length, X_scaled.shape[1])\n",
    "output_dim = y_sourceid_encoded.shape[1]\n",
    "model = transformer_model(input_shape, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fed0869-09f8-467d-9320-851b7c1a1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ccf9d9de-9dae-49e4-819a-e1cf0c2e777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_5\" is incompatible with the layer: expected shape=(None, 41, 4), found shape=(None, 42, 4)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filehnmo4tfq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_5\" is incompatible with the layer: expected shape=(None, 41, 4), found shape=(None, 42, 4)\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(X_sequences, y_sequences, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "400533ec-23a9-4c61-9526-c5405d77724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 152s 1s/step\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "866043f9-78b9-4156-97fd-d4c4b9823bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted probabilities back to class indices\n",
    "predicted_classes = np.argmax(predicted_sourceids, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b74dde7-84af-4727-b4ad-9c317cc0ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few predicted sourceIDs:\n",
      "[[4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot back to original sourceID using the encoder\n",
    "predicted_sourceids_final = encoder.inverse_transform(predicted_sourceids)\n",
    "# Print results\n",
    "print(\"\\nFirst few predicted sourceIDs:\")\n",
    "print(predicted_sourceids_final[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c533e0e-6323-4f2a-9801-37e48cc59d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function that respects start and stop tokens\n",
    "def predict_with_tokens(model, input_sequence, start_token, stop_token, max_length=100):\n",
    "    # Initialize the output sequence with the start token\n",
    "    predicted_sequence = [start_token]\n",
    "    \n",
    "    # Predict step-by-step until we hit the stop token or reach max length\n",
    "    for _ in range(max_length):\n",
    "        # Use the current sequence to predict the next token\n",
    "        current_input = np.array(predicted_sequence[-sequence_length:]).reshape(1, -1)\n",
    "        next_token_probs = model.predict(current_input)\n",
    "        next_token = np.argmax(next_token_probs)\n",
    "        \n",
    "        # Append the predicted token to the sequence\n",
    "        predicted_sequence.append(next_token)\n",
    "        \n",
    "        # If the stop token is predicted, stop generating\n",
    "        if next_token == stop_token:\n",
    "            break\n",
    "    \n",
    "    return predicted_sequence\n",
    "\n",
    "# Test prediction with tokens\n",
    "sample_input_sequence = X_sequences[0]  # Use the first sequence for demonstration\n",
    "predicted_sequence = predict_with_tokens(model, sample_input_sequence, start_token, stop_token)\n",
    "print(\"Predicted sequence with start and stop tokens:\", predicted_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c29ed2-9b12-4256-a3db-708510c68af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map predicted token sequence to original sourceIDs using the legend\n",
    "def map_tokens_to_sourceids(token_sequence, encoding_legend):\n",
    "    sourceids = [encoding_legend.get(token, 'Unknown') for token in token_sequence]\n",
    "    return sourceids\n",
    "\n",
    "# Convert the predicted sequence to sourceIDs\n",
    "predicted_sourceid_sequence = map_tokens_to_sourceids(predicted_sequence, encoding_legend)\n",
    "print(\"Predicted sourceID sequence:\", predicted_sourceid_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35aed398-e9d2-4d89-bb71-adb8cfda4ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few predicted sourceIDs:\n",
      "['MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257']\n",
      "Predicted sourceIDs saved to 'predicted_sourceids_final_1.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Function to map one-hot encoded predictions back to sourceIDs\n",
    "def map_onehot_to_sourceid(onehot_predictions, encoding_legend):\n",
    "    sourceids = []\n",
    "    for prediction in onehot_predictions:\n",
    "        index = np.argmax(prediction)  # Find the index of the highest value\n",
    "        sourceid = encoding_legend[index + 1]  # Map back using the legend (1-based index)\n",
    "        sourceids.append(sourceid)\n",
    "    return sourceids\n",
    "\n",
    "# Encoding legend mapping (this is just a sample, adjust to your actual encoding legend)\n",
    "encoding_legend = {\n",
    "    1: 'MRI_CCS_11',\n",
    "    2: 'MRI_EXU_95',\n",
    "    3: 'MRI_FRR_18',\n",
    "    4: 'MRI_FRR_257',\n",
    "    5: 'MRI_FRR_264',\n",
    "    6: 'MRI_FRR_3',\n",
    "    7: 'MRI_FRR_34',\n",
    "    8: 'MRI_MPT_1005',\n",
    "    9: 'MRI_MSR_100',\n",
    "    10: 'MRI_MSR_104',\n",
    "    11: 'MRI_MSR_21',\n",
    "    12: 'MRI_MSR_34'\n",
    "}\n",
    "\n",
    "# Map predicted one-hot encodings to original sourceIDs\n",
    "predicted_sourceids_final = map_onehot_to_sourceid(predicted_sourceids, encoding_legend)\n",
    "\n",
    "# Print the final predicted sourceIDs\n",
    "print(\"\\nFirst few predicted sourceIDs:\")\n",
    "print(predicted_sourceids_final[:32])\n",
    "\n",
    "# Optional: Save the predicted sourceIDs to a CSV file\n",
    "output_df = pd.DataFrame(predicted_sourceids_final, columns=[\"Predicted SourceID\"])\n",
    "output_df.to_csv(\"predicted_sourceids_final_1.csv\", index=False)\n",
    "print(\"Predicted sourceIDs saved to 'predicted_sourceids_final_1.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcd71f-98b4-49f3-941b-7af27940e924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a6645-d76c-4788-85ea-72e762f61eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4e5f1-5e1f-4db5-8b9f-a8bd49b2a915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
