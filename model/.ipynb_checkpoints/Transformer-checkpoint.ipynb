{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a52984ef-399a-482a-9f4a-aa1f8b9b0b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Embedding, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15304c14-cf87-4a5d-8c71-22d0fb9ec508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "learning_rate = 0.01\n",
    "key_dim = 16\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83f61e6d-43e0-412a-854a-fa1c92840451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from ../data/encoded_176398_HEAD.csv.\n",
      "\n",
      "First 5 rows of the dataset before any processing:\n",
      "              datetime  sourceID  timediff  ZAxisInPossible  ZAxisOutPossible  \\\n",
      "0  2023-03-27 08:14:34        10       0.0              NaN               NaN   \n",
      "1  2023-03-27 08:14:49         4      15.0              NaN               NaN   \n",
      "2  2023-03-27 08:14:56         5      22.0              1.0               0.0   \n",
      "3  2023-03-27 08:15:08         1      34.0              1.0               0.0   \n",
      "4  2023-03-27 08:15:39        12      65.0              1.0               0.0   \n",
      "\n",
      "   YAxisDownPossible  YAxisUpPossible       PTAB  BC  S1  ...  C24  EN  SHL  \\\n",
      "0                NaN              NaN        NaN   0 NaN  ...  NaN NaN  NaN   \n",
      "1                NaN              NaN -1127700.0   0 NaN  ...  NaN NaN  NaN   \n",
      "2                1.0              1.0 -1127700.0   0 NaN  ...  NaN NaN  NaN   \n",
      "3                1.0              1.0 -1127700.0   1 NaN  ...  NaN NaN  NaN   \n",
      "4                1.0              1.0 -1127700.0   1 NaN  ...  NaN NaN  NaN   \n",
      "\n",
      "   SHS  BodyPart_from  BodyPart_to                            PatientID_from  \\\n",
      "0  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "1  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "2  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "3  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "4  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "\n",
      "                               PatientID_to  BodyGroup_from  BodyGroup_to  \n",
      "0  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "1  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "2  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "3  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "4  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "\n",
      "[5 rows x 122 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the already encoded data\n",
    "file_path = \"../data/encoded_176398_HEAD.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Data loaded successfully from {file_path}.\\n\")\n",
    "print(\"First 5 rows of the dataset before any processing:\")\n",
    "print(df.head())  # Print first few rows to understand the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf2dad6e-dc1d-44a1-85c6-6f9f54471a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped unnecessary columns.\n",
      "Remaining columns: ['sourceID', 'timediff', 'PTAB', 'BodyGroup_from', 'BodyGroup_to']\n",
      "   sourceID  timediff       PTAB  BodyGroup_from  BodyGroup_to\n",
      "0        10       0.0        NaN               1             4\n",
      "1         4      15.0 -1127700.0               1             4\n",
      "2         5      22.0 -1127700.0               1             4\n",
      "3         1      34.0 -1127700.0               1             4\n",
      "4        12      65.0 -1127700.0               1             4\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns (based on your latest specification)\n",
    "columns_to_drop = ['datetime', 'SN', 'ZAxisInPossible', 'ZAxisOutPossible', 'YAxisDownPossible', \n",
    "                   'YAxisUpPossible', 'BC', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', \n",
    "                   'S5', 'S6', 'S7', 'S8', 'S9', 'BO1', 'BO2', 'BO3', 'B1', 'B2', 'B3', 'B4', \n",
    "                   'B5', 'HE2', 'HE4', 'NE2', 'HE1', 'HE3', 'NE1', 'SHA', 'HW1', 'HW2', 'HW3', \n",
    "                   '18K', 'FA', 'TO', 'BAL', 'BAR', 'BCL', 'BCR', 'HC2', 'HC4', 'HC6', 'HC7', \n",
    "                   'NC2', 'HC1', 'HC3', 'HC5', 'NC1', 'Na', 'UFL', 'PA1', 'PA2', 'PA3', 'PA4', \n",
    "                   'PA5', 'PA6', 'SP1', 'SP2', 'SP3', 'SP4', 'SP5', 'SP6', 'SP7', 'SP8', 'BL8', \n",
    "                   'BR8', 'UFS', 'HEA', 'HEP', 'SC', 'PeH', 'PeN', 'FS', 'FL', 'BY1', 'BY2', \n",
    "                   'BY3', 'BL', 'BR', 'HE', 'BL4', 'BR4', 'BL1', 'BR1', 'BL2', 'BR2', 'L7', \n",
    "                   'L4', 'H2L', 'N2L', 'H1U', 'N1U', 'He1', 'He2', 'TR1', 'TR2', 'TR3', 'TR4', \n",
    "                   'TR5', 'TR6', 'MR', 'ML', 'BL5', 'BR5', 'C24', 'EN', 'SHL', 'SHS','BodyPart_from', \n",
    "                   'BodyPart_to', 'PatientID_from', 'PatientID_to']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "print(\"Dropped unnecessary columns.\")\n",
    "print(\"Remaining columns:\", df.columns.tolist())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1397fae-e907-4020-b74b-3d16be7a0707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sourceID  timediff       PTAB  BodyGroup_from  BodyGroup_to\n",
      "0        10       0.0        0.0               1             4\n",
      "1         4      15.0 -1127700.0               1             4\n",
      "2         5      22.0 -1127700.0               1             4\n",
      "3         1      34.0 -1127700.0               1             4\n",
      "4        12      65.0 -1127700.0               1             4\n"
     ]
    }
   ],
   "source": [
    "# Replace NaN values with 0 in the DataFrame\n",
    "df.fillna(0, inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1904656c-1090-4e8a-97d3-8f53cb5a1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target (sourceID)\n",
    "X = df.drop(columns=['sourceID'])\n",
    "y_sourceid = df['sourceID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78bdf0b2-71d9-4890-bdc5-603a45a1c2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few one-hot encoded 'sourceID' values:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Shape of one-hot encoded 'sourceID': (4501, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode 'sourceID'\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_sourceid_encoded = encoder.fit_transform(y_sourceid.values.reshape(-1, 1))\n",
    "original_sourceids = encoder.categories_[0]\n",
    "# Print the first few rows of y_sourceid_encoded to confirm one-hot encoding worked\n",
    "print(\"\\nFirst few one-hot encoded 'sourceID' values:\")\n",
    "print(y_sourceid_encoded[:5])\n",
    "print(\"Shape of one-hot encoded 'sourceID':\", y_sourceid_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ce7e72e-eb07-4a28-8e1d-f55f0755d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff40f546-c53a-4ede-882c-6a475efc375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sequence length of 5 for Transformer input\n",
    "sequence_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "655d282c-1adf-424f-8888-2f2032944cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        targets.append(target[i+seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "X_sequences, y_sequences = create_sequences(X_scaled, y_sourceid_encoded, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f863f69-8264-4adc-834e-00ba1562ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder model\n",
    "def transformer_model(input_shape, output_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Embedding layer to transform input to higher dimensional space (if needed)\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    \n",
    "    # Multi-head attention layer\n",
    "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    \n",
    "    # Add & Norm\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "    \n",
    "    # Feedforward layer\n",
    "    ff_dim = 256  # You can adjust this dimension as needed\n",
    "    x_ffn = Dense(ff_dim, activation='relu')(x)\n",
    "    x_ffn = Dense(x.shape[-1])(x_ffn) \n",
    "    \n",
    "    # Final classification layer (softmax for multi-class)\n",
    "    outputs = Dense(output_dim, activation='softmax')(x[:, -1, :])  # Only last timestep output\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "108f88fd-26fb-4358-bf34-fdfa2b755cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "input_shape = (sequence_length, X_scaled.shape[1])\n",
    "output_dim = y_sourceid_encoded.shape[1]\n",
    "model = transformer_model(input_shape, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fed0869-09f8-467d-9320-851b7c1a1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ccf9d9de-9dae-49e4-819a-e1cf0c2e777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "126/126 [==============================] - 73s 555ms/step - loss: 1.9540\n",
      "Epoch 2/30\n",
      "126/126 [==============================] - 70s 558ms/step - loss: 1.8875\n",
      "Epoch 3/30\n",
      "126/126 [==============================] - 69s 546ms/step - loss: 1.8761\n",
      "Epoch 4/30\n",
      "126/126 [==============================] - 70s 553ms/step - loss: 1.8587\n",
      "Epoch 5/30\n",
      "126/126 [==============================] - 69s 547ms/step - loss: 1.8588\n",
      "Epoch 6/30\n",
      "126/126 [==============================] - 69s 547ms/step - loss: 1.8514\n",
      "Epoch 7/30\n",
      "126/126 [==============================] - 71s 562ms/step - loss: 1.8540\n",
      "Epoch 8/30\n",
      "126/126 [==============================] - 70s 558ms/step - loss: 1.8463\n",
      "Epoch 9/30\n",
      "126/126 [==============================] - 72s 569ms/step - loss: 1.8303\n",
      "Epoch 10/30\n",
      "126/126 [==============================] - 73s 582ms/step - loss: 1.8204\n",
      "Epoch 11/30\n",
      "126/126 [==============================] - 78s 619ms/step - loss: 1.8156\n",
      "Epoch 12/30\n",
      "126/126 [==============================] - 80s 631ms/step - loss: 1.8066\n",
      "Epoch 13/30\n",
      "126/126 [==============================] - 76s 607ms/step - loss: 1.8099\n",
      "Epoch 14/30\n",
      "126/126 [==============================] - 74s 590ms/step - loss: 1.7946\n",
      "Epoch 15/30\n",
      "126/126 [==============================] - 73s 581ms/step - loss: 1.8024\n",
      "Epoch 16/30\n",
      "126/126 [==============================] - 74s 584ms/step - loss: 1.7865\n",
      "Epoch 17/30\n",
      "126/126 [==============================] - 78s 621ms/step - loss: 1.7786\n",
      "Epoch 18/30\n",
      "126/126 [==============================] - 80s 635ms/step - loss: 1.7953\n",
      "Epoch 19/30\n",
      "126/126 [==============================] - 74s 585ms/step - loss: 1.7702\n",
      "Epoch 20/30\n",
      "126/126 [==============================] - 74s 584ms/step - loss: 1.7578\n",
      "Epoch 21/30\n",
      "126/126 [==============================] - 70s 557ms/step - loss: 1.7582\n",
      "Epoch 22/30\n",
      "126/126 [==============================] - 70s 550ms/step - loss: 1.7506\n",
      "Epoch 23/30\n",
      "126/126 [==============================] - 70s 552ms/step - loss: 1.7456\n",
      "Epoch 24/30\n",
      "126/126 [==============================] - 68s 542ms/step - loss: 1.7428\n",
      "Epoch 25/30\n",
      "126/126 [==============================] - 70s 555ms/step - loss: 1.7275\n",
      "Epoch 26/30\n",
      "126/126 [==============================] - 72s 570ms/step - loss: 1.7281\n",
      "Epoch 27/30\n",
      "126/126 [==============================] - 69s 550ms/step - loss: 1.7187\n",
      "Epoch 28/30\n",
      "126/126 [==============================] - 68s 538ms/step - loss: 1.7258\n",
      "Epoch 29/30\n",
      "126/126 [==============================] - 68s 537ms/step - loss: 1.7442\n",
      "Epoch 30/30\n",
      "126/126 [==============================] - 68s 541ms/step - loss: 1.7123\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(X_sequences, y_sequences, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "400533ec-23a9-4c61-9526-c5405d77724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 35s 277ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict on training data\n",
    "predicted_sourceids = model.predict(X_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "866043f9-78b9-4156-97fd-d4c4b9823bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted probabilities back to class indices\n",
    "predicted_classes = np.argmax(predicted_sourceids, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b74dde7-84af-4727-b4ad-9c317cc0ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few predicted sourceIDs:\n",
      "[[5]\n",
      " [5]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [5]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [5]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot back to original sourceID using the encoder\n",
    "predicted_sourceids_final = encoder.inverse_transform(predicted_sourceids)\n",
    "# Print results\n",
    "print(\"\\nFirst few predicted sourceIDs:\")\n",
    "print(predicted_sourceids_final[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35aed398-e9d2-4d89-bb71-adb8cfda4ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few predicted sourceIDs:\n",
      "['MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_257', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_264', 'MRI_FRR_257', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_257', 'MRI_FRR_264', 'MRI_FRR_257', 'MRI_CCS_11', 'MRI_CCS_11', 'MRI_CCS_11']\n",
      "Predicted sourceIDs saved to 'predicted_sourceids_final_lower_lr.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Function to map one-hot encoded predictions back to sourceIDs\n",
    "def map_onehot_to_sourceid(onehot_predictions, encoding_legend):\n",
    "    sourceids = []\n",
    "    for prediction in onehot_predictions:\n",
    "        index = np.argmax(prediction)  # Find the index of the highest value\n",
    "        sourceid = encoding_legend[index + 1]  # Map back using the legend (1-based index)\n",
    "        sourceids.append(sourceid)\n",
    "    return sourceids\n",
    "\n",
    "# Encoding legend mapping (this is just a sample, adjust to your actual encoding legend)\n",
    "encoding_legend = {\n",
    "    1: 'MRI_CCS_11',\n",
    "    2: 'MRI_EXU_95',\n",
    "    3: 'MRI_FRR_18',\n",
    "    4: 'MRI_FRR_257',\n",
    "    5: 'MRI_FRR_264',\n",
    "    6: 'MRI_FRR_3',\n",
    "    7: 'MRI_FRR_34',\n",
    "    8: 'MRI_MPT_1005',\n",
    "    9: 'MRI_MSR_100',\n",
    "    10: 'MRI_MSR_104',\n",
    "    11: 'MRI_MSR_21',\n",
    "    12: 'MRI_MSR_34'\n",
    "}\n",
    "\n",
    "# Map predicted one-hot encodings to original sourceIDs\n",
    "predicted_sourceids_final = map_onehot_to_sourceid(predicted_sourceids, encoding_legend)\n",
    "\n",
    "# Print the final predicted sourceIDs\n",
    "print(\"\\nFirst few predicted sourceIDs:\")\n",
    "print(predicted_sourceids_final[:32])\n",
    "\n",
    "# Optional: Save the predicted sourceIDs to a CSV file\n",
    "output_df = pd.DataFrame(predicted_sourceids_final, columns=[\"Predicted SourceID\"])\n",
    "output_df.to_csv(\"predicted_sourceids_final_1.csv\", index=False)\n",
    "print(\"Predicted sourceIDs saved to 'predicted_sourceids_final_1.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcd71f-98b4-49f3-941b-7af27940e924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a6645-d76c-4788-85ea-72e762f61eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4e5f1-5e1f-4db5-8b9f-a8bd49b2a915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
