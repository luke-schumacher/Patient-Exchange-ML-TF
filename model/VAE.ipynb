{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed927349-8c52-45c6-93d8-0fb28bd5513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0552ae-b0da-41c6-b64d-5554bcab8b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from ../data/encoded_176398_HEAD.csv.\n",
      "\n",
      "Columns in DataFrame: ['datetime', 'sourceID', 'timediff', 'ZAxisInPossible', 'ZAxisOutPossible', 'YAxisDownPossible', 'YAxisUpPossible', 'PTAB', 'BC', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'BO1', 'BO2', 'BO3', 'B1', 'B2', 'B3', 'B4', 'B5', 'HE2', 'HE4', 'NE2', 'HE1', 'HE3', 'NE1', 'SHA', 'HW1', 'HW2', 'HW3', '18K', 'FA', 'TO', 'BAL', 'BAR', 'BCL', 'BCR', 'SN', 'HC2', 'HC4', 'HC6', 'HC7', 'NC2', 'HC1', 'HC3', 'HC5', 'NC1', 'Na', 'UFL', 'PA1', 'PA2', 'PA3', 'PA4', 'PA5', 'PA6', 'SP1', 'SP2', 'SP3', 'SP4', 'SP5', 'SP6', 'SP7', 'SP8', 'BL8', 'BR8', 'UFS', 'HEA', 'HEP', 'SC', 'PeH', 'PeN', 'FS', 'FL', 'BY1', 'BY2', 'BY3', 'BL', 'BR', 'HE', 'BL4', 'BR4', 'BL1', 'BR1', 'BL2', 'BR2', 'L7', 'L4', 'H2L', 'N2L', 'H1U', 'N1U', 'He1', 'He2', 'TR1', 'TR2', 'TR3', 'TR4', 'TR5', 'TR6', 'MR', 'ML', 'BL5', 'BR5', 'C24', 'EN', 'SHL', 'SHS', 'BodyPart_from', 'BodyPart_to', 'PatientID_from', 'PatientID_to', 'BodyGroup_from', 'BodyGroup_to']\n"
     ]
    }
   ],
   "source": [
    "# Load the already encoded data\n",
    "file_path = \"../data/encoded_176398_HEAD.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Data loaded successfully from {file_path}.\\n\")\n",
    "print(\"Columns in DataFrame:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b057265a-a02c-40e7-b3f2-8af05d08e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 'datetime', 'PatientID' and 'BodyPart' columns\n"
     ]
    }
   ],
   "source": [
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=['datetime','PatientID_from', 'PatientID_to', 'SN', 'BodyPart_from', 'BodyPart_to'])\n",
    "print(\"Dropped 'datetime', 'PatientID' and 'BodyPart' columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2166a2-a5af-466a-b100-e5a53e83a54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape after dropping columns: (4501, 116)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the dataframe after dropping columns\n",
    "print(\"DataFrame shape after dropping columns:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8020facb-3630-42b4-9ce5-277fd72e2566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated features and targets\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target columns\n",
    "X = df.drop(columns=['BodyGroup_from', 'BodyGroup_to', 'sourceID'])\n",
    "y_bodygroup_from = df['BodyGroup_from']\n",
    "y_bodygroup_to = df['BodyGroup_to']\n",
    "y_sourceid = df['sourceID']\n",
    "print(\"Separated features and targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523f9308-9092-4dc0-8b56-e98e096682bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit sourceID to the first 30 unique values\n",
    "unique_source_ids = y_sourceid.unique()[:30]\n",
    "y_sourceid = y_sourceid[y_sourceid.isin(unique_source_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9269588-d85b-4f65-be92-ed9797478399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled features shape: (4501, 113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:480: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:481: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Scaled features shape:\", X_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6a325b-57eb-49af-a685-ecec7daed5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE architecture\n",
    "input_dim = X_scaled.shape[1]\n",
    "latent_dim = 2  # Latent space dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e37141-e070-40ef-ac99-ad45280fdd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "inputs = Input(shape=(input_dim,))\n",
    "h = Dense(64, activation='relu')(inputs)\n",
    "h = Dense(32, activation='relu')(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d5cd078-80ba-480f-87c8-3747cdadcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79ae315d-0c72-41a4-be56-4e37bbfde2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_h = Dense(32, activation='relu')\n",
    "decoder_mean = Dense(64, activation='relu')\n",
    "outputs = Dense(input_dim, activation='sigmoid')(decoder_mean(decoder_h(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d984f47-2be3-45c8-8857-a228b4e5c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "vae = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ab7976-0b46-4b80-86ed-d40e3ffd71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def vae_loss(y_true, y_pred):\n",
    "    reconstruction_loss = K.mean(K.square(y_true - y_pred))\n",
    "    kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "    return reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eba0eab4-7b2c-4e73-8b9f-260e06b3fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "vae.compile(optimizer='adam', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f0339eb-fd56-469b-b870-2a4a020ee0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 329, in __call__\n        self._total_loss_mean.update_state(\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 509, in update_state  **\n        sample_weight = tf.__internal__.ops.broadcast_weights(\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\keras_tensor.py\", line 285, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=\"created by layer 'tf.cast_2'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAE model training completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileb3dcppek.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 329, in __call__\n        self._total_loss_mean.update_state(\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 509, in update_state  **\n        sample_weight = tf.__internal__.ops.broadcast_weights(\n    File \"C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\keras_tensor.py\", line 285, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=\"created by layer 'tf.cast_2'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "vae.fit(X_scaled, X_scaled, epochs=100, batch_size=32)\n",
    "print(\"VAE model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80ab60c1-d1c0-4442-9b1a-c908a5c195eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 0s 2ms/step\n",
      "Encoded data shape: (4501, 113)\n"
     ]
    }
   ],
   "source": [
    "# Encode the data using the trained VAE\n",
    "encoded_data = vae.predict(X_scaled)\n",
    "\n",
    "# Print the encoded data shape\n",
    "print(\"Encoded data shape:\", encoded_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "208d5050-d143-4984-a05e-c00573a670e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'BodyGroup_from'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine the encoded data with original labels\u001b[39;00m\n\u001b[0;32m      2\u001b[0m encoded_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(encoded_data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoded_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(encoded_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])])\n\u001b[1;32m----> 3\u001b[0m encoded_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBodyGroup_from\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43my_bodygroup_from\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBodyGroup_from\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m encoded_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBodyGroup_to\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_bodygroup_from[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBodyGroup_to\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m encoded_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msourceID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_sourceid\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:418\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'BodyGroup_from'"
     ]
    }
   ],
   "source": [
    "# Combine the encoded data with original labels\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=[f'encoded_{i}' for i in range(encoded_data.shape[1])])\n",
    "encoded_df['BodyGroup_from'] = y_bodygroup_from['BodyGroup_from'].reset_index(drop=True)\n",
    "encoded_df['BodyGroup_to'] = y_bodygroup_from['BodyGroup_to'].reset_index(drop=True)\n",
    "encoded_df['sourceID'] = y_sourceid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f73b4dc-db0d-466f-9766-282a4fa0169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data saved to ../data/predicted_output.csv.\n"
     ]
    }
   ],
   "source": [
    "# Output the predicted data\n",
    "output_file = \"../data/predicted_output.csv\"\n",
    "encoded_df.to_csv(output_file, index=False)\n",
    "print(f\"Predicted data saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d55b31-c62d-4096-b8eb-8088b67c539d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
