{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "ed927349-8c52-45c6-93d8-0fb28bd5513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "#from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "842325de-a9fe-42e8-b463-eef3c6c4e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_eager_execution()\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "7d0552ae-b0da-41c6-b64d-5554bcab8b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from ../data/encoded_176398_HEAD.csv.\n",
      "\n",
      "First 5 rows of the dataset before any processing:\n",
      "              datetime  sourceID  timediff  ZAxisInPossible  ZAxisOutPossible  \\\n",
      "0  2023-03-27 08:14:34        10       0.0              NaN               NaN   \n",
      "1  2023-03-27 08:14:49         4      15.0              NaN               NaN   \n",
      "2  2023-03-27 08:14:56         5      22.0              1.0               0.0   \n",
      "3  2023-03-27 08:15:08         1      34.0              1.0               0.0   \n",
      "4  2023-03-27 08:15:39        12      65.0              1.0               0.0   \n",
      "\n",
      "   YAxisDownPossible  YAxisUpPossible       PTAB  BC  S1  ...  C24  EN  SHL  \\\n",
      "0                NaN              NaN        NaN   0 NaN  ...  NaN NaN  NaN   \n",
      "1                NaN              NaN -1127700.0   0 NaN  ...  NaN NaN  NaN   \n",
      "2                1.0              1.0 -1127700.0   0 NaN  ...  NaN NaN  NaN   \n",
      "3                1.0              1.0 -1127700.0   1 NaN  ...  NaN NaN  NaN   \n",
      "4                1.0              1.0 -1127700.0   1 NaN  ...  NaN NaN  NaN   \n",
      "\n",
      "   SHS  BodyPart_from  BodyPart_to                            PatientID_from  \\\n",
      "0  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "1  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "2  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "3  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "4  NaN          BRAIN        BRAIN  80416a5e946f12b0d3e0fabce7ff76b6c95c2476   \n",
      "\n",
      "                               PatientID_to  BodyGroup_from  BodyGroup_to  \n",
      "0  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "1  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "2  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "3  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "4  6e81762c9c2cf2534a1789063381d4e610184a5b               1             4  \n",
      "\n",
      "[5 rows x 122 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the already encoded data\n",
    "file_path = \"../data/encoded_176398_HEAD.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Data loaded successfully from {file_path}.\\n\")\n",
    "print(\"First 5 rows of the dataset before any processing:\")\n",
    "print(df.head())  # Print first few rows to understand the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "b057265a-a02c-40e7-b3f2-8af05d08e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped unnecessary columns.\n",
      "Remaining columns: ['sourceID', 'timediff', 'PTAB', 'BodyGroup_from', 'BodyGroup_to']\n",
      "   sourceID  timediff       PTAB  BodyGroup_from  BodyGroup_to\n",
      "0        10       0.0        NaN               1             4\n",
      "1         4      15.0 -1127700.0               1             4\n",
      "2         5      22.0 -1127700.0               1             4\n",
      "3         1      34.0 -1127700.0               1             4\n",
      "4        12      65.0 -1127700.0               1             4\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns (based on your latest specification)\n",
    "columns_to_drop = ['datetime', 'SN', 'ZAxisInPossible', 'ZAxisOutPossible', 'YAxisDownPossible', \n",
    "                   'YAxisUpPossible', 'BC', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', \n",
    "                   'S5', 'S6', 'S7', 'S8', 'S9', 'BO1', 'BO2', 'BO3', 'B1', 'B2', 'B3', 'B4', \n",
    "                   'B5', 'HE2', 'HE4', 'NE2', 'HE1', 'HE3', 'NE1', 'SHA', 'HW1', 'HW2', 'HW3', \n",
    "                   '18K', 'FA', 'TO', 'BAL', 'BAR', 'BCL', 'BCR', 'HC2', 'HC4', 'HC6', 'HC7', \n",
    "                   'NC2', 'HC1', 'HC3', 'HC5', 'NC1', 'Na', 'UFL', 'PA1', 'PA2', 'PA3', 'PA4', \n",
    "                   'PA5', 'PA6', 'SP1', 'SP2', 'SP3', 'SP4', 'SP5', 'SP6', 'SP7', 'SP8', 'BL8', \n",
    "                   'BR8', 'UFS', 'HEA', 'HEP', 'SC', 'PeH', 'PeN', 'FS', 'FL', 'BY1', 'BY2', \n",
    "                   'BY3', 'BL', 'BR', 'HE', 'BL4', 'BR4', 'BL1', 'BR1', 'BL2', 'BR2', 'L7', \n",
    "                   'L4', 'H2L', 'N2L', 'H1U', 'N1U', 'He1', 'He2', 'TR1', 'TR2', 'TR3', 'TR4', \n",
    "                   'TR5', 'TR6', 'MR', 'ML', 'BL5', 'BR5', 'C24', 'EN', 'SHL', 'SHS','BodyPart_from', \n",
    "                   'BodyPart_to', 'PatientID_from', 'PatientID_to']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "print(\"Dropped unnecessary columns.\")\n",
    "print(\"Remaining columns:\", df.columns.tolist())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "978c461d-9d70-454e-b174-d3b30cdede1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values replaced with 0.\n",
      "   sourceID  timediff       PTAB  BodyGroup_from  BodyGroup_to\n",
      "0        10       0.0        0.0               1             4\n",
      "1         4      15.0 -1127700.0               1             4\n",
      "2         5      22.0 -1127700.0               1             4\n",
      "3         1      34.0 -1127700.0               1             4\n",
      "4        12      65.0 -1127700.0               1             4\n"
     ]
    }
   ],
   "source": [
    "# Replace NaN values with 0 in the DataFrame\n",
    "df.fillna(0, inplace=True)\n",
    "print(\"NaN values replaced with 0.\")\n",
    "print(df.head())  # Display a sample of the updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "da2166a2-a5af-466a-b100-e5a53e83a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target (sourceID)\n",
    "X = df.drop(columns=['sourceID'])\n",
    "y_sourceid = df['sourceID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "8020facb-3630-42b4-9ce5-277fd72e2566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode 'sourceID' if needed\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_sourceid_encoded = encoder.fit_transform(y_sourceid.values.reshape(-1, 1))\n",
    "original_sourceids = encoder.categories_[0]  # Save the mapping of one-hot encoding to original sourceIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "2aa7d937-cc83-4fb7-80c9-3be3b7c3e5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few one-hot encoded 'sourceID' values:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "(4501, 4)\n",
      "Shape of one-hot encoded 'sourceID': (4501, 12)\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows of y_sourceid_encoded to confirm one-hot encoding worked\n",
    "print(\"\\nFirst few one-hot encoded 'sourceID' values:\")\n",
    "print(y_sourceid_encoded[:5])\n",
    "print(X_scaled.shape)\n",
    "print(\"Shape of one-hot encoded 'sourceID':\", y_sourceid_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "523f9308-9092-4dc0-8b56-e98e096682bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled features shape: (4501, 4)\n",
      "First 5 rows of scaled data:\n",
      "[[0.         0.99947446 0.         0.27272727]\n",
      " [0.00119847 0.43504592 0.         0.27272727]\n",
      " [0.00175775 0.43504592 0.         0.27272727]\n",
      " [0.00271652 0.43504592 0.         0.27272727]\n",
      " [0.00519335 0.43504592 0.         0.27272727]]\n"
     ]
    }
   ],
   "source": [
    "# Scale the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"\\nScaled features shape:\", X_scaled.shape)\n",
    "print(\"First 5 rows of scaled data:\")\n",
    "print(X_scaled[:5])  # Print the first 5 rows after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "b9269588-d85b-4f65-be92-ed9797478399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4501, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define the VAE architecture\n",
    "input_dim = X_scaled.shape[1]\n",
    "latent_dim = 2  # Latent space dimension\n",
    "# Check the shape of X_scaled\n",
    "print(X_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "aa6a325b-57eb-49af-a685-ecec7daed5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "inputs = Input(shape=(input_dim,))\n",
    "h = Dense(128, activation='relu')(inputs) \n",
    "h = Dense(64, activation='relu')(h) \n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "b0e37141-e070-40ef-ac99-ad45280fdd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "6d5cd078-80ba-480f-87c8-3747cdadcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_h = Dense(64, activation='relu')\n",
    "decoder_mean = Dense(128, activation='relu')\n",
    "outputs = Dense(12, activation='softmax')(decoder_mean(decoder_h(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "79ae315d-0c72-41a4-be56-4e37bbfde2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "vae = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "8d984f47-2be3-45c8-8857-a228b4e5c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE loss function\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    reconstruction_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.categorical_crossentropy(x, x_decoded_mean)\n",
    "    )\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "    return reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "16de7c67-9881-4c12-8f07-1ba73ad89a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the output layer matches the input\n",
    "outputs = Dense(X_scaled.shape[1], activation='sigmoid')(decoder_mean(decoder_h(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "d0ab7976-0b46-4b80-86ed-d40e3ffd71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the legacy optimizer\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile model\n",
    "vae.compile(optimizer=optimizer, loss=vae_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "9f0339eb-fd56-469b-b870-2a4a020ee0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Train on 4501 samples\n",
      "Epoch 1/100\n",
      "4501/4501 [==============================] - 1s 227us/sample - loss: 2.0183\n",
      "Epoch 2/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9779\n",
      "Epoch 3/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9795\n",
      "Epoch 4/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9749\n",
      "Epoch 5/100\n",
      "4501/4501 [==============================] - 0s 53us/sample - loss: 1.9737\n",
      "Epoch 6/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9759\n",
      "Epoch 7/100\n",
      "4501/4501 [==============================] - 0s 54us/sample - loss: 1.9728\n",
      "Epoch 8/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9727\n",
      "Epoch 9/100\n",
      "4501/4501 [==============================] - 0s 54us/sample - loss: 1.9748\n",
      "Epoch 10/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9709\n",
      "Epoch 11/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9726\n",
      "Epoch 12/100\n",
      "4501/4501 [==============================] - 0s 56us/sample - loss: 1.9707\n",
      "Epoch 13/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9723\n",
      "Epoch 14/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9711\n",
      "Epoch 15/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9717\n",
      "Epoch 16/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9707\n",
      "Epoch 17/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9703\n",
      "Epoch 18/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9695\n",
      "Epoch 19/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9706\n",
      "Epoch 20/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9709\n",
      "Epoch 21/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9719\n",
      "Epoch 22/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9706\n",
      "Epoch 23/100\n",
      "4501/4501 [==============================] - 0s 53us/sample - loss: 1.9707\n",
      "Epoch 24/100\n",
      "4501/4501 [==============================] - 0s 53us/sample - loss: 1.9695\n",
      "Epoch 25/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9704\n",
      "Epoch 26/100\n",
      "4501/4501 [==============================] - 0s 55us/sample - loss: 1.9711\n",
      "Epoch 27/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9702\n",
      "Epoch 28/100\n",
      "4501/4501 [==============================] - 0s 57us/sample - loss: 1.9693\n",
      "Epoch 29/100\n",
      "4501/4501 [==============================] - 0s 55us/sample - loss: 1.9708\n",
      "Epoch 30/100\n",
      "4501/4501 [==============================] - 0s 54us/sample - loss: 1.9698\n",
      "Epoch 31/100\n",
      "4501/4501 [==============================] - 0s 54us/sample - loss: 1.9703\n",
      "Epoch 32/100\n",
      "4501/4501 [==============================] - 0s 54us/sample - loss: 1.9706\n",
      "Epoch 33/100\n",
      "4501/4501 [==============================] - 0s 58us/sample - loss: 1.9694\n",
      "Epoch 34/100\n",
      "4501/4501 [==============================] - 0s 53us/sample - loss: 1.9689\n",
      "Epoch 35/100\n",
      "4501/4501 [==============================] - 0s 60us/sample - loss: 1.9691\n",
      "Epoch 36/100\n",
      "4501/4501 [==============================] - 0s 58us/sample - loss: 1.9698\n",
      "Epoch 37/100\n",
      "4501/4501 [==============================] - 0s 55us/sample - loss: 1.9703\n",
      "Epoch 38/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9691\n",
      "Epoch 39/100\n",
      "4501/4501 [==============================] - 0s 58us/sample - loss: 1.9686\n",
      "Epoch 40/100\n",
      "4501/4501 [==============================] - 0s 60us/sample - loss: 1.9698\n",
      "Epoch 41/100\n",
      "4501/4501 [==============================] - 0s 61us/sample - loss: 1.9689\n",
      "Epoch 42/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9695\n",
      "Epoch 43/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9692\n",
      "Epoch 44/100\n",
      "4501/4501 [==============================] - 0s 53us/sample - loss: 1.9688\n",
      "Epoch 45/100\n",
      "4501/4501 [==============================] - 0s 53us/sample - loss: 1.9697\n",
      "Epoch 46/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9693\n",
      "Epoch 47/100\n",
      "4501/4501 [==============================] - 0s 54us/sample - loss: 1.9694\n",
      "Epoch 48/100\n",
      "4501/4501 [==============================] - 0s 60us/sample - loss: 1.9686\n",
      "Epoch 49/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9691\n",
      "Epoch 50/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9686\n",
      "Epoch 51/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9693\n",
      "Epoch 52/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9687\n",
      "Epoch 53/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9687\n",
      "Epoch 54/100\n",
      "4501/4501 [==============================] - 0s 54us/sample - loss: 1.9682\n",
      "Epoch 55/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9684\n",
      "Epoch 56/100\n",
      "4501/4501 [==============================] - 0s 54us/sample - loss: 1.9681\n",
      "Epoch 57/100\n",
      "4501/4501 [==============================] - 0s 56us/sample - loss: 1.9687\n",
      "Epoch 58/100\n",
      "4501/4501 [==============================] - 0s 67us/sample - loss: 1.9681\n",
      "Epoch 59/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9684\n",
      "Epoch 60/100\n",
      "4501/4501 [==============================] - 0s 46us/sample - loss: 1.9683\n",
      "Epoch 61/100\n",
      "4501/4501 [==============================] - 0s 47us/sample - loss: 1.9684\n",
      "Epoch 62/100\n",
      "4501/4501 [==============================] - 0s 46us/sample - loss: 1.9681\n",
      "Epoch 63/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9680\n",
      "Epoch 64/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9682\n",
      "Epoch 65/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9676\n",
      "Epoch 66/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9691\n",
      "Epoch 67/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9684\n",
      "Epoch 68/100\n",
      "4501/4501 [==============================] - 0s 47us/sample - loss: 1.9683\n",
      "Epoch 69/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9683\n",
      "Epoch 70/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9684\n",
      "Epoch 71/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9686\n",
      "Epoch 72/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9674\n",
      "Epoch 73/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9684\n",
      "Epoch 74/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9679\n",
      "Epoch 75/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9680\n",
      "Epoch 76/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9681\n",
      "Epoch 77/100\n",
      "4501/4501 [==============================] - 0s 47us/sample - loss: 1.9677\n",
      "Epoch 78/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9683\n",
      "Epoch 79/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9678\n",
      "Epoch 80/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9690\n",
      "Epoch 81/100\n",
      "4501/4501 [==============================] - 0s 52us/sample - loss: 1.9687\n",
      "Epoch 82/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9679\n",
      "Epoch 83/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9679\n",
      "Epoch 84/100\n",
      "4501/4501 [==============================] - 0s 47us/sample - loss: 1.9677\n",
      "Epoch 85/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9675\n",
      "Epoch 86/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9679\n",
      "Epoch 87/100\n",
      "4501/4501 [==============================] - 0s 53us/sample - loss: 1.9677\n",
      "Epoch 88/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9678\n",
      "Epoch 89/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9678\n",
      "Epoch 90/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9672\n",
      "Epoch 91/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9678\n",
      "Epoch 92/100\n",
      "4501/4501 [==============================] - 0s 51us/sample - loss: 1.9675\n",
      "Epoch 93/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9674\n",
      "Epoch 94/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9676\n",
      "Epoch 95/100\n",
      "4501/4501 [==============================] - 0s 64us/sample - loss: 1.9673\n",
      "Epoch 96/100\n",
      "4501/4501 [==============================] - 0s 49us/sample - loss: 1.9677\n",
      "Epoch 97/100\n",
      "4501/4501 [==============================] - 0s 48us/sample - loss: 1.9679\n",
      "Epoch 98/100\n",
      "4501/4501 [==============================] - 0s 59us/sample - loss: 1.9676\n",
      "Epoch 99/100\n",
      "4501/4501 [==============================] - 0s 47us/sample - loss: 1.9679\n",
      "Epoch 100/100\n",
      "4501/4501 [==============================] - 0s 50us/sample - loss: 1.9675\n",
      "VAE model training completed.\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "print(\"\\nStarting model training...\")\n",
    "# Fit the model with one-hot encoded labels\n",
    "history = vae.fit(X_scaled, y_sourceid_encoded, epochs=100, batch_size=32)\n",
    "print(\"VAE model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "80ab60c1-d1c0-4442-9b1a-c908a5c195eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in sourceID:\n",
      "sourceID\n",
      "5     1208\n",
      "4     1012\n",
      "1      698\n",
      "12     335\n",
      "8      331\n",
      "10     329\n",
      "9      329\n",
      "7      125\n",
      "2      115\n",
      "3        9\n",
      "6        6\n",
      "11       4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution in sourceID:\")\n",
    "print(y_sourceid.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "48edc777-14cb-40c1-b0a5-68fc1ab5ec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of predicted one-hot encoded 'sourceID' values: (4501, 12)\n",
      "First few predicted one-hot encoded 'sourceID' values:\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the training data\n",
    "predicted_sourceids = vae.predict(X_scaled)\n",
    "\n",
    "# Convert predictions to one-hot encoded format\n",
    "# Using softmax output, we'll simply threshold at 0.5 for binary (or interpret probabilities directly for multi-class)\n",
    "predicted_sourceids_onehot = (predicted_sourceids == predicted_sourceids.max(axis=1, keepdims=True)).astype(float)\n",
    "\n",
    "# Check shape and print first few rows\n",
    "print(\"\\nShape of predicted one-hot encoded 'sourceID' values:\", predicted_sourceids_onehot.shape)\n",
    "print(\"First few predicted one-hot encoded 'sourceID' values:\")\n",
    "print(predicted_sourceids_onehot[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "e7639a5d-4fcf-4fd1-9ca0-c267a474cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted class distribution:\n",
      "{4: 4501}\n"
     ]
    }
   ],
   "source": [
    "# Convert predicted probabilities back to class indices\n",
    "predicted_classes = np.argmax(predicted_sourceids_onehot, axis=1)\n",
    "\n",
    "# Create a mapping back to original sourceIDs\n",
    "sourceID_mapping = encoder.inverse_transform(predicted_sourceids_onehot)\n",
    "\n",
    "# Count predicted classes\n",
    "unique, counts = np.unique(predicted_classes, return_counts=True)\n",
    "predicted_class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "# Print the distribution of predicted classes\n",
    "print(\"\\nPredicted class distribution:\")\n",
    "print(predicted_class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "208d5050-d143-4984-a05e-c00573a670e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few predicted sourceIDs:\n",
      "['MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264', 'MRI_FRR_264']\n"
     ]
    }
   ],
   "source": [
    "# Function to map one-hot encoded predictions back to sourceIDs\n",
    "def map_onehot_to_sourceid(onehot_predictions, encoding_legend):\n",
    "    sourceids = []\n",
    "    for prediction in onehot_predictions:\n",
    "        index = np.argmax(prediction)  # Find the index of the highest value\n",
    "        sourceid = encoding_legend[index + 1]  # Map back using the legend (1-based index)\n",
    "        sourceids.append(sourceid)\n",
    "    return sourceids\n",
    "\n",
    "# Encoding legend mapping\n",
    "encoding_legend = {\n",
    "    1: 'MRI_CCS_11',\n",
    "    2: 'MRI_EXU_95',\n",
    "    3: 'MRI_FRR_18',\n",
    "    4: 'MRI_FRR_257',\n",
    "    5: 'MRI_FRR_264',\n",
    "    6: 'MRI_FRR_3',\n",
    "    7: 'MRI_FRR_34',\n",
    "    8: 'MRI_MPT_1005',\n",
    "    9: 'MRI_MSR_100',\n",
    "    10: 'MRI_MSR_104',\n",
    "    11: 'MRI_MSR_21',\n",
    "    12: 'MRI_MSR_34'\n",
    "}\n",
    "\n",
    "# Map predicted one-hot encodings to original sourceIDs\n",
    "predicted_sourceids_final = map_onehot_to_sourceid(predicted_sourceids_onehot, encoding_legend)\n",
    "\n",
    "# Print the final predicted sourceIDs\n",
    "print(\"\\nFirst few predicted sourceIDs:\")\n",
    "print(predicted_sourceids_final[:16])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
