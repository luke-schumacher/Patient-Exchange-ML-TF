{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006a3a14-7b1a-45e3-97d0-85b5329dfcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Embedding, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2709bda6-9e25-43a4-b318-f219fcf894e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "key_dim = 16\n",
    "num_heads = 8\n",
    "sequence_length = 1000  # Adjust as needed\n",
    "start_token = [0]  # You can customize start token based on your input format\n",
    "end_token = [0]  # You can customize end token based on your input format\n",
    "data_directory = \"../data/filtered_blocks/\"  # Directory containing block CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b71011-561f-4a38-a571-973f3a60982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from 4493 rows across all blocks.\n"
     ]
    }
   ],
   "source": [
    "# Load all block CSV files and concatenate them\n",
    "def load_blocks(data_dir):\n",
    "    all_data = []\n",
    "    for filename in sorted(os.listdir(data_dir)):\n",
    "        if filename.startswith(\"block_\") and filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            block_data = pd.read_csv(file_path)\n",
    "            all_data.append(block_data)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "df = load_blocks(data_directory)\n",
    "print(f\"Loaded data from {len(df)} rows across all blocks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642f0a7a-af33-4ad2-bff5-525751692310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped unnecessary columns. Remaining columns: ['sourceID', 'timediff', 'PTAB', 'BodyGroup_from', 'BodyGroup_to']\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['datetime', 'SN', 'ZAxisInPossible', 'ZAxisOutPossible', 'YAxisDownPossible',\n",
    "                   'YAxisUpPossible', 'BC', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4',\n",
    "                   'S5', 'S6', 'S7', 'S8', 'S9', 'BO1', 'BO2', 'BO3', 'B1', 'B2', 'B3', 'B4',\n",
    "                   'B5', 'HE2', 'HE4', 'NE2', 'HE1', 'HE3', 'NE1', 'SHA', 'HW1', 'HW2', 'HW3',\n",
    "                   '18K', 'FA', 'TO', 'BAL', 'BAR', 'BCL', 'BCR', 'HC2', 'HC4', 'HC6', 'HC7',\n",
    "                   'NC2', 'HC1', 'HC3', 'HC5', 'NC1', 'Na', 'UFL', 'PA1', 'PA2', 'PA3', 'PA4',\n",
    "                   'PA5', 'PA6', 'SP1', 'SP2', 'SP3', 'SP4', 'SP5', 'SP6', 'SP7', 'SP8', 'BL8',\n",
    "                   'BR8', 'UFS', 'HEA', 'HEP', 'SC', 'PeH', 'PeN', 'FS', 'FL', 'BY1', 'BY2',\n",
    "                   'BY3', 'BL', 'BR', 'HE', 'BL4', 'BR4', 'BL1', 'BR1', 'BL2', 'BR2', 'L7',\n",
    "                   'L4', 'H2L', 'N2L', 'H1U', 'N1U', 'He1', 'He2', 'TR1', 'TR2', 'TR3', 'TR4',\n",
    "                   'TR5', 'TR6', 'MR', 'ML', 'BL5', 'BR5', 'C24', 'EN', 'SHL', 'SHS', 'BodyPart_from',\n",
    "                   'BodyPart_to', 'PatientID_from', 'PatientID_to']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "print(f\"Dropped unnecessary columns. Remaining columns: {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d7ffac-b092-4f22-a939-928c68b3a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2f61c2-87c8-4c8c-8281-109fa724314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target (sourceID)\n",
    "X = df.drop(columns=['sourceID'])\n",
    "y_sourceid = df['sourceID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc4b3cf-7e8a-4c98-941c-84f2ea37afde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode 'sourceID'\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_sourceid_encoded = encoder.fit_transform(y_sourceid.values.reshape(-1, 1))\n",
    "original_sourceids = encoder.categories_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e269d0-70f7-420f-860f-0b92148e15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a30e002-8d59-41c8-83d5-149b8320dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sequence creation to include start and end tokens\n",
    "def create_sequences(data, target, seq_length, start_token, end_token):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length].tolist()\n",
    "        seq.insert(0, start_token * len(seq[0]))  # Add start token\n",
    "        seq.append(end_token * len(seq[0]))  # Add end token\n",
    "        sequences.append(seq)\n",
    "        targets.append(target[i+seq_length])\n",
    "    return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5d46e-f340-467f-981f-0079fc09f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated version***\n",
    "def create_sequences(data, target, seq_length, start_token=10, end_token=9):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length].tolist()\n",
    "        # Add start token\n",
    "        seq = [[start_token] * len(seq[0])] + seq  \n",
    "        # Add end token\n",
    "        seq.append([end_token] * len(seq[0]))  \n",
    "        sequences.append(seq)\n",
    "        targets.append(target[i + seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Call the updated function\n",
    "X_sequences, y_sequences = create_sequences(X_scaled, y_sourceid_encoded, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624d17fc-194e-4588-859d-c6b5e3ed74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences with start and end tokens\n",
    "X_sequences, y_sequences = create_sequences(X_scaled, y_sourceid_encoded, sequence_length, start_token, end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9590809-0e13-4105-a4ce-d9057fea854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder model\n",
    "def transformer_model(input_shape, output_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Dense layer\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    \n",
    "    # Multi-head attention layer\n",
    "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    \n",
    "    # Add & Norm\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "    \n",
    "    # Feedforward layer\n",
    "    ff_dim = 256  # adjust this dimension as needed\n",
    "    x_ffn = Dense(ff_dim, activation='relu')(x)\n",
    "    x_ffn = Dense(x.shape[-1])(x_ffn) \n",
    "    \n",
    "    # Final classification layer (softmax for multi-class)\n",
    "    outputs = Dense(output_dim, activation='softmax')(x[:, -1, :])  # Only last timestep output\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da7f1e0b-b84a-4bbb-baa8-b05c17d05744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "input_shape = (sequence_length + 2, X_scaled.shape[1])  # Adjust for start and end tokens\n",
    "output_dim = y_sourceid_encoded.shape[1]\n",
    "model = transformer_model(input_shape, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ccf4bfc-962f-44eb-94d6-41cdaa43b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff93af7-24a3-4ac6-bc3f-f0ca5faac3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\z004uyxr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "110/110 [==============================] - 316s 3s/step - loss: 2.2828\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 322s 3s/step - loss: 2.0271\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - 312s 3s/step - loss: 2.0035\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - 322s 3s/step - loss: 2.0003\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 330s 3s/step - loss: 1.9952\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 334s 3s/step - loss: 1.9870\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 317s 3s/step - loss: 1.9758\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 318s 3s/step - loss: 1.9787\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 329s 3s/step - loss: 1.9792\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 322s 3s/step - loss: 1.9798\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 324s 3s/step - loss: 1.9748\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 314s 3s/step - loss: 1.9804\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - 322s 3s/step - loss: 1.9749\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 327s 3s/step - loss: 1.9768\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 332s 3s/step - loss: 1.9749\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - 324s 3s/step - loss: 1.9763\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 331s 3s/step - loss: 1.9791\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 313s 3s/step - loss: 1.9765\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 312s 3s/step - loss: 1.9743\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 287s 3s/step - loss: 1.9780\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 256s 2s/step - loss: 1.9741\n",
      "Epoch 22/30\n",
      " 67/110 [=================>............] - ETA: 1:44 - loss: 1.9662"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(X_sequences, y_sequences, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0550430-8a70-4876-9438-dd13e433265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training data\n",
    "predicted_sourceids = model.predict(X_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b705f7-2919-483b-bf07-5d70aa88a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot back to original sourceID using the encoder\n",
    "predicted_sourceids_final = encoder.inverse_transform(predicted_sourceids)\n",
    "\n",
    "# Print the predicted sourceIDs\n",
    "print(\"Predicted SourceIDs:\")\n",
    "print(predicted_sourceids_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208271d-90df-4304-a559-ed330258b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert one-hot back to original sourceID using the encoder\n",
    "predicted_sourceids_final = encoder.inverse_transform(predicted_sourceids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
